{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages and functions\n",
    "from matplotlib.pylab import figure, semilogx, loglog, xlabel, ylabel, legend, title, subplot, show, grid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Add the path to the toolbox_02450 module\n",
    "sys.path.append('../Tools')\n",
    "from toolbox_02450 import rlr_validate, correlated_ttest, train_neural_net, draw_neural_net\n",
    "\n",
    "# Import custom functions from data_preprocessing module\n",
    "from data_preprocessing import *\n",
    "dfjoint, dfRec, dfClas = dataPreprocess()\n",
    "colNamesMeans, colNamesStd, colNamesExt, colNamesOther = getSpecificColNames()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# n_left = 33 + 28 + 30 + 29\n",
    "# n_right = 4 + 2 + 3 + 5 + 1\n",
    "# n_total = n_right + n_left\n",
    "\n",
    "# p1 = (33 + 28 + 30 + 29)/n_total\n",
    "# p2 = (4 + 2 + 3 + 5)/n_total\n",
    "# p3 = 0\n",
    "# p4 = 0.075\n",
    "\n",
    "# lp = 1 - max(p1,p2,p3,p4)\n",
    "# ll = 1 - 1/120\n",
    "# lr = 1 - (n_left + n_right - 1) / 120\n",
    "\n",
    "# print(lp - ((n_left / n_total) * ll) - ((n_right / n_total) * lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "mean = dfjoint.loc[:, colNamesMeans]\n",
    "sde = dfjoint.loc[:, colNamesStd]\n",
    "worst = dfjoint.loc[:, colNamesExt]\n",
    "colNames = colNamesMeans + colNamesStd + colNamesExt\n",
    "\n",
    "# Discretize time variable\n",
    "time_discretized = []\n",
    "for i in range(len(dfRec.iloc[:,0])):\n",
    "    if dfRec['time'].iloc[i] <= 12:\n",
    "        time_discretized.append('<1 years')\n",
    "    elif dfRec['time'].iloc[i] <= 36:\n",
    "        time_discretized.append('1-3 years')\n",
    "    elif dfRec['time'].iloc[i] <= 72:\n",
    "        time_discretized.append('>3-6 years')\n",
    "    else:\n",
    "        time_discretized.append('6+ years')\n",
    "\n",
    "# Add discretized time variable to dataframe\n",
    "dfRec['time_discretized'] = time_discretized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare data for classification\n",
    "attributeNames = [u'Offset'] + colNames\n",
    "X = dfRec.loc[:, colNames]\n",
    "classLabels = dfRec.loc[:,\"time_discretized\"]\n",
    "classNames = sorted(set(dfRec.loc[:,\"time_discretized\"]))\n",
    "\n",
    "# Create a dictionary mapping class names to integer labels\n",
    "classDict = dict(zip(classNames, range(4)))\n",
    "y = np.asarray([classDict[value] for value in classLabels])\n",
    "\n",
    "# Define data dimensions\n",
    "N = X.shape[0]\n",
    "M = len(attributeNames)\n",
    "# M = M + 1\n",
    "C = len(classNames)\n",
    "\n",
    "# Add bias to input data\n",
    "X = np.concatenate((np.ones((X.shape[0],1)), X), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------\n",
      "Linear regression model:\n",
      "\n",
      "Outer fold 1\n",
      "has optimal lambda: 100000000.0\n",
      "and optimal generalization error: 1.3944463669710239\n",
      "Outer fold 2\n",
      "has optimal lambda: 100000000.0\n",
      "and optimal generalization error: 1.3151280030474635\n",
      "Outer fold 3\n",
      "has optimal lambda: 100000000.0\n",
      "and optimal generalization error: 1.3026407263290802\n",
      "Outer fold 4\n",
      "has optimal lambda: 100000000.0\n",
      "and optimal generalization error: 1.3295992318489356\n",
      "Outer fold 5\n",
      "has optimal lambda: 100000000.0\n",
      "and optimal generalization error: 1.319924906087703\n",
      "Outer fold 6\n",
      "has optimal lambda: 100000000.0\n",
      "and optimal generalization error: 1.354987883181662\n",
      "Outer fold 7\n",
      "has optimal lambda: 100000000.0\n",
      "and optimal generalization error: 1.364567409247599\n",
      "Outer fold 8\n",
      "has optimal lambda: 100000000.0\n",
      "and optimal generalization error: 1.303477759600937\n",
      "Outer fold 9\n",
      "has optimal lambda: 100000000.0\n",
      "and optimal generalization error: 1.328650710466178\n",
      "Outer fold 10\n",
      "has optimal lambda: 10000.0\n",
      "and optimal generalization error: 1.3093824919587096\n"
     ]
    }
   ],
   "source": [
    "# 2 level cross validation, linear regression model\n",
    "\n",
    "# Define number of folds for cross-validation\n",
    "K = 10\n",
    "\n",
    "# Create cross-validation partition for evaluation\n",
    "CV = model_selection.KFold(K, shuffle=True)\n",
    "#CV = model_selection.KFold(K, shuffle=False)\n",
    "\n",
    "# Define lambda values to be tested\n",
    "lambdas = np.power(10.,range(-5,9))\n",
    "\n",
    "# Initialize variables to store errors and weights\n",
    "Error_train = np.empty((K,1))\n",
    "Error_test = np.empty((K,1))\n",
    "Error_train_rlr = np.empty((K,1))\n",
    "Error_test_rlr = np.empty((K,1))\n",
    "Error_train_nofeatures = np.empty((K,1))\n",
    "Error_test_nofeatures = np.empty((K,1))\n",
    "w_rlr = np.empty((M,K))\n",
    "mu = np.empty((K, M-1))\n",
    "sigma = np.empty((K, M-1))\n",
    "w_noreg = np.empty((M,K))\n",
    "errors_lin_reg = np.empty((K,1))\n",
    "\n",
    "# Loop over each fold in the cross-validation\n",
    "k = 0\n",
    "print('\\n-----------------------')\n",
    "print('Linear regression model:\\n')\n",
    "\n",
    "for train_index, test_index in CV.split(X, y):\n",
    "\n",
    "    # Extract training and test set for current CV fold\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "    # Define the number of internal folds for cross-validation\n",
    "    internal_cross_validation = 10\n",
    "\n",
    "    # Validate the performance of the linear regression model using ridge regression\n",
    "    opt_val_err, opt_lambda, mean_w_vs_lambda, train_err_vs_lambda, test_err_vs_lambda = rlr_validate(X_train, y_train, lambdas, internal_cross_validation)\n",
    "\n",
    "    # Standardize the data based on the training set and save the mean and standard deviation\n",
    "    # since they're part of the model\n",
    "    mu[k, :] = np.mean(X_train[:, 1:], 0)\n",
    "    sigma[k, :] = np.std(X_train[:, 1:], 0)\n",
    "\n",
    "    X_train[:, 1:] = (X_train[:, 1:] - mu[k, :]) / sigma[k, :]\n",
    "    X_test[:, 1:] = (X_test[:, 1:] - mu[k, :]) / sigma[k, :]\n",
    "\n",
    "    # Save the generalization error for later statistics\n",
    "    errors_lin_reg[k] = opt_val_err\n",
    "\n",
    "    k += 1\n",
    "\n",
    "    # Print the results of the current fold\n",
    "    print('Outer fold {0}'.format(k))\n",
    "    print('has optimal lambda: {0}'.format(opt_lambda))\n",
    "    print('and optimal generalization error: {0}'.format(opt_val_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------\n",
      "Baseline model:\n",
      "\n",
      "Outer fold 1 generalization error: [0.66795455]\n",
      "Outer fold 2 generalization error: [0.77755455]\n",
      "Outer fold 3 generalization error: [0.80232727]\n",
      "Outer fold 4 generalization error: [0.71923636]\n",
      "Outer fold 5 generalization error: [0.94634216]\n",
      "Outer fold 6 generalization error: [0.95872954]\n",
      "Outer fold 7 generalization error: [1.00376667]\n",
      "Outer fold 8 generalization error: [1.01416667]\n",
      "Outer fold 9 generalization error: [0.96]\n",
      "Outer fold 10 generalization error: [1.07773333]\n"
     ]
    }
   ],
   "source": [
    "# 2 level Cross validation, baseline\n",
    "\n",
    "# Define number of outer and inner folds for cross-validation\n",
    "K = 10\n",
    "K_i = 10\n",
    "\n",
    "# Create cross-validation partition for evaluation\n",
    "CV = model_selection.KFold(K, shuffle=True)\n",
    "#CV = model_selection.KFold(K, shuffle=False)\n",
    "\n",
    "# Initialize variables to store results\n",
    "mu = np.empty((K, M-1))\n",
    "sigma = np.empty((K, M-1))\n",
    "error_i = np.empty((K_i, 1))\n",
    "error_o = np.empty((K, 1))\n",
    "\n",
    "print('\\n-----------------------')\n",
    "print('Baseline model:\\n')\n",
    "\n",
    "# Loop over outer cross-validation folds\n",
    "k=0\n",
    "for train_index, test_index in CV.split(X,y):\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    \n",
    "    CV_i = model_selection.KFold(K_i, shuffle=True)\n",
    "    \n",
    "    k_i = 0\n",
    "    for train_index, test_index in CV_i.split(X_train,y_train):\n",
    "        \n",
    "        # extract training and test set for current inner CV fold\n",
    "        X_train_i = X_train[train_index]\n",
    "        y_train_i = y_train[train_index]\n",
    "        X_test_i = X_train[test_index]\n",
    "        y_test_i = y_train[test_index]\n",
    "        \n",
    "        # calculate mean squared error for inner CV fold\n",
    "        mean_i = np.mean(y_train_i)\n",
    "        error_i[k_i]=(1/len(y_test_i))*np.sum((y_test_i-mean_i)**2)\n",
    "        #Error_i[k_i] = (1/len(y_test_i)*\n",
    "        \n",
    "        k_i+=1\n",
    "    # find minimum error for inner CV folds\n",
    "    error_o[k] = min(error_i)\n",
    "        \n",
    "    # Standardize outer fold based on training set, and save the mean and standard\n",
    "    # deviations since they're part of the model (they would be needed for\n",
    "    # making new predictions) - for brevity we won't always store these in the scripts\n",
    "    mu[k, :] = np.mean(X_train[:, 1:], 0)\n",
    "    sigma[k, :] = np.std(X_train[:, 1:], 0)\n",
    "    \n",
    "    X_train[:, 1:] = (X_train[:, 1:] - mu[k, :] ) / sigma[k, :] \n",
    "    X_test[:, 1:] = (X_test[:, 1:] - mu[k, :] ) / sigma[k, :] \n",
    "    \n",
    "    print('Outer fold {} generalization error: {}'.format(k+1,error_o[k]))\n",
    "\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\py10\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([99])) that is different to the input size (torch.Size([99, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tFinal loss:\n",
      "\t\t933\t1.3177229\t9.951265e-07\n",
      "\n",
      "\tBest loss: 1.3177229166030884\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\py10\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t1.6931305\t0.0010116483\n",
      "\t\t2000\t1.2616706\t1.0015338e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2293\t1.2601532\t9.459896e-07\n",
      "\n",
      "\tBest loss: 1.2601531744003296\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.5436608\t0.00087226776\n",
      "\t\t2000\t1.2837561\t1.300035e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2005\t1.2837487\t8.3574196e-07\n",
      "\n",
      "\tBest loss: 1.283748745918274\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t395\t1.283631\t8.3581864e-07\n",
      "\n",
      "\tBest loss: 1.2836309671401978\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t618\t1.213947\t9.819964e-07\n",
      "\n",
      "\tBest loss: 1.213947057723999\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3630148\t5.7723273e-06\n",
      "\t\t2000\t1.353111\t8.63374e-06\n",
      "\t\t3000\t1.3408313\t9.157337e-06\n",
      "\t\t4000\t1.3296641\t7.082591e-06\n",
      "\t\t5000\t1.3228036\t3.3343792e-06\n",
      "\t\tFinal loss:\n",
      "\t\t5720\t1.3207512\t9.0258624e-07\n",
      "\n",
      "\tBest loss: 1.3207511901855469\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t44\t1.2756183\t5.6071326e-07\n",
      "\n",
      "\tBest loss: 1.275618314743042\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t647\t1.2118183\t9.837215e-07\n",
      "\n",
      "\tBest loss: 1.2118183374404907\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t124\t1.2579033\t9.4768154e-07\n",
      "\n",
      "\tBest loss: 1.2579033374786377\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.2325264\t5.8031337e-06\n",
      "\t\t2000\t1.2244875\t6.620058e-06\n",
      "\t\t3000\t1.2175336\t4.6017713e-06\n",
      "\t\t4000\t1.2140273\t1.3747036e-06\n",
      "\t\tFinal loss:\n",
      "\t\t4122\t1.2138236\t9.820964e-07\n",
      "\n",
      "\tBest loss: 1.2138235569000244\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t4.310689\t0.0011450155\n",
      "\t\t2000\t1.6915278\t0.0005831167\n",
      "\t\t3000\t1.3225797\t4.299203e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3762\t1.3096833\t9.1021377e-07\n",
      "\n",
      "\tBest loss: 1.3096833229064941\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t4.44602\t0.0015218151\n",
      "\t\t2000\t1.505416\t0.00044831375\n",
      "\t\t3000\t1.3225068\t6.760371e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3278\t1.3213108\t9.924242e-07\n",
      "\n",
      "\tBest loss: 1.3213107585906982\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3412272\t1.1465487e-05\n",
      "\t\t2000\t1.3377918\t2.22772e-06\n",
      "\t\t3000\t1.3342332\t2.9484313e-06\n",
      "\t\t4000\t1.3298554\t3.4959785e-06\n",
      "\t\t5000\t1.325571\t3.0576284e-06\n",
      "\t\t6000\t1.3225793\t1.5322747e-06\n",
      "\t\tFinal loss:\n",
      "\t\t6199\t1.3222032\t9.917545e-07\n",
      "\n",
      "\tBest loss: 1.3222031593322754\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t618\t1.2976484\t9.186555e-07\n",
      "\n",
      "\tBest loss: 1.2976484298706055\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3911065\t2.1165937e-05\n",
      "\t\t2000\t1.357743\t2.554903e-05\n",
      "\t\t3000\t1.3265961\t1.922989e-05\n",
      "\t\t4000\t1.3083662\t8.564557e-06\n",
      "\t\t5000\t1.3026239\t1.4642337e-06\n",
      "\t\tFinal loss:\n",
      "\t\t5134\t1.3023924\t9.153093e-07\n",
      "\n",
      "\tBest loss: 1.3023923635482788\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3172885\t2.3528894e-06\n",
      "\t\t2000\t1.3133854\t3.4490542e-06\n",
      "\t\t3000\t1.3086047\t3.5527503e-06\n",
      "\t\t4000\t1.3043724\t2.7417543e-06\n",
      "\t\tFinal loss:\n",
      "\t\t4881\t1.3021145\t9.155046e-07\n",
      "\n",
      "\tBest loss: 1.302114486694336\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t535\t1.2579395\t9.476543e-07\n",
      "\n",
      "\tBest loss: 1.2579394578933716\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t457\t1.3180175\t9.94904e-07\n",
      "\n",
      "\tBest loss: 1.3180174827575684\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t159\t1.3059074\t9.128456e-07\n",
      "\n",
      "\tBest loss: 1.3059073686599731\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t37\t1.3676888\t8.716113e-08\n",
      "\n",
      "\tBest loss: 1.367688775062561\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.4429241\t0.00013547264\n",
      "\t\t2000\t1.4071449\t3.7275402e-06\n",
      "\t\t3000\t1.4005861\t5.447257e-06\n",
      "\t\t4000\t1.3915653\t7.1958593e-06\n",
      "\t\t5000\t1.380944\t7.682821e-06\n",
      "\t\t6000\t1.3709297\t6.4346336e-06\n",
      "\t\t7000\t1.3641655\t3.3206659e-06\n",
      "\t\tFinal loss:\n",
      "\t\t7868\t1.3615978\t9.630604e-07\n",
      "\n",
      "\tBest loss: 1.3615977764129639\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.4390111\t0.00044060338\n",
      "\t\t2000\t1.3197079\t4.064836e-06\n",
      "\t\t3000\t1.3139635\t5.26202e-06\n",
      "\t\t4000\t1.3057873\t7.120806e-06\n",
      "\t\t5000\t1.2954817\t8.465702e-06\n",
      "\t\t6000\t1.2846733\t7.980174e-06\n",
      "\t\t7000\t1.2761049\t5.231298e-06\n",
      "\t\t8000\t1.2716835\t1.8748229e-06\n",
      "\t\tFinal loss:\n",
      "\t\t8295\t1.2711048\t9.378391e-07\n",
      "\n",
      "\tBest loss: 1.2711048126220703\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.4992932\t0.0004955012\n",
      "\t\tFinal loss:\n",
      "\t\t1823\t1.3780324\t8.6506805e-07\n",
      "\n",
      "\tBest loss: 1.3780324459075928\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3584365\t5.6162744e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1115\t1.3579936\t7.900499e-07\n",
      "\n",
      "\tBest loss: 1.3579936027526855\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.356869\t1.6340977e-05\n",
      "\t\t2000\t1.3429012\t4.5272477e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2625\t1.3407562\t8.89119e-07\n",
      "\n",
      "\tBest loss: 1.3407561779022217\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3047072\t3.5267043e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1265\t1.3011867\t9.161574e-07\n",
      "\n",
      "\tBest loss: 1.3011866807937622\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t747\t1.277712\t9.3298945e-07\n",
      "\n",
      "\tBest loss: 1.2777119874954224\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3105807\t3.8202693e-06\n",
      "\t\t2000\t1.3072103\t1.4590963e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2046\t1.3071319\t9.1199047e-07\n",
      "\n",
      "\tBest loss: 1.3071318864822388\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t75\t1.3704307\t9.568531e-07\n",
      "\n",
      "\tBest loss: 1.3704307079315186\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t49\t1.4232478\t4.1879298e-07\n",
      "\n",
      "\tBest loss: 1.4232478141784668\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t315\t1.3627608\t9.622386e-07\n",
      "\n",
      "\tBest loss: 1.3627607822418213\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t288\t1.3576306\t8.780679e-07\n",
      "\n",
      "\tBest loss: 1.3576306104660034\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t265\t1.3016242\t8.242646e-07\n",
      "\n",
      "\tBest loss: 1.3016241788864136\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3750532\t1.10967485e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1170\t1.3740015\t9.543664e-07\n",
      "\n",
      "\tBest loss: 1.3740015029907227\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.7944887\t0.0013981377\n",
      "\t\t2000\t1.3357768\t1.3386496e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2019\t1.3357444\t8.9245503e-07\n",
      "\n",
      "\tBest loss: 1.3357443809509277\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3631536\t4.1101853e-06\n",
      "\t\t2000\t1.3562448\t6.1527235e-06\n",
      "\t\t3000\t1.347735\t6.2800223e-06\n",
      "\t\t4000\t1.3401028\t4.7146104e-06\n",
      "\t\t5000\t1.3355614\t2.142183e-06\n",
      "\t\tFinal loss:\n",
      "\t\t5379\t1.3347284\t9.824478e-07\n",
      "\n",
      "\tBest loss: 1.3347283601760864\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t535\t1.3731457\t8.681467e-07\n",
      "\n",
      "\tBest loss: 1.3731456995010376\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t163\t1.3779113\t9.516584e-07\n",
      "\n",
      "\tBest loss: 1.3779113292694092\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t757\t1.3147494\t9.0670653e-07\n",
      "\n",
      "\tBest loss: 1.3147493600845337\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t754\t1.401663\t9.355323e-07\n",
      "\n",
      "\tBest loss: 1.4016629457473755\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3107595\t2.610103e-05\n",
      "\t\t2000\t1.3051189\t1.8267925e-06\n",
      "\t\t3000\t1.3021063\t2.6549756e-06\n",
      "\t\t4000\t1.2983147\t3.1218187e-06\n",
      "\t\t5000\t1.2944614\t2.6706552e-06\n",
      "\t\t6000\t1.2916032\t1.5690227e-06\n",
      "\t\tFinal loss:\n",
      "\t\t6378\t1.2909346\t9.2343316e-07\n",
      "\n",
      "\tBest loss: 1.2909345626831055\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t292\t1.3259344\t9.889636e-07\n",
      "\n",
      "\tBest loss: 1.3259344100952148\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\py10\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([101])) that is different to the input size (torch.Size([101, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t1.2998723\t3.924968e-05\n",
      "\t\t2000\t1.2881203\t6.5706627e-06\n",
      "\t\t3000\t1.2796206\t6.1485134e-06\n",
      "\t\t4000\t1.2731792\t3.6516033e-06\n",
      "\t\tFinal loss:\n",
      "\t\t4927\t1.2704023\t9.383577e-07\n",
      "\n",
      "\tBest loss: 1.2704023122787476\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t3.2133062\t0.0020826745\n",
      "\t\t2000\t1.2947308\t7.899209e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2632\t1.2788689\t9.321454e-07\n",
      "\n",
      "\tBest loss: 1.2788689136505127\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.2375689\t1.9264704e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1205\t1.2358567\t9.645873e-07\n",
      "\n",
      "\tBest loss: 1.2358566522598267\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3961852\t2.9370596e-05\n",
      "\t\t2000\t1.3557853\t2.593761e-05\n",
      "\t\t3000\t1.330277\t1.1828694e-05\n",
      "\t\t4000\t1.3215511\t2.9767255e-06\n",
      "\t\tFinal loss:\n",
      "\t\t4897\t1.3193475\t9.93901e-07\n",
      "\n",
      "\tBest loss: 1.3193475008010864\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t666\t1.2352971\t9.650244e-07\n",
      "\n",
      "\tBest loss: 1.2352970838546753\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t67\t1.27811\t7.4615923e-07\n",
      "\n",
      "\tBest loss: 1.2781100273132324\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t62\t1.3050679\t8.220896e-07\n",
      "\n",
      "\tBest loss: 1.3050678968429565\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t109\t1.230077\t9.691196e-07\n",
      "\n",
      "\tBest loss: 1.2300770282745361\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t322\t1.2339381\t9.660871e-07\n",
      "\n",
      "\tBest loss: 1.2339380979537964\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.2338028\t9.082141e-06\n",
      "\t\t2000\t1.2223129\t8.777411e-06\n",
      "\t\t3000\t1.2140405\t4.7132025e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3954\t1.2109641\t9.844155e-07\n",
      "\n",
      "\tBest loss: 1.2109640836715698\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.2616123\t1.8897892e-06\n",
      "\t\t2000\t1.2595091\t1.987592e-06\n",
      "\t\t3000\t1.2566459\t2.5612962e-06\n",
      "\t\t4000\t1.2534788\t2.4726655e-06\n",
      "\t\t5000\t1.2508725\t1.6201129e-06\n",
      "\t\tFinal loss:\n",
      "\t\t5374\t1.2502034\t8.581665e-07\n",
      "\n",
      "\tBest loss: 1.2502033710479736\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t63\t1.2049799\t4.9465234e-07\n",
      "\n",
      "\tBest loss: 1.2049798965454102\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t497\t1.1875805\t8.030391e-07\n",
      "\n",
      "\tBest loss: 1.1875804662704468\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t58\t1.238506\t5.7751464e-07\n",
      "\n",
      "\tBest loss: 1.2385059595108032\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t660\t1.2893062\t9.245995e-07\n",
      "\n",
      "\tBest loss: 1.2893061637878418\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t979\t1.2573456\t9.4810196e-07\n",
      "\n",
      "\tBest loss: 1.2573455572128296\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t48\t1.2445863\t1.9156448e-07\n",
      "\n",
      "\tBest loss: 1.2445863485336304\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t287\t1.2191304\t7.8225725e-07\n",
      "\n",
      "\tBest loss: 1.2191303968429565\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t2.4900634\t0.001024506\n",
      "\t\t2000\t1.4554452\t0.0001339798\n",
      "\t\tFinal loss:\n",
      "\t\t2974\t1.4077516\t9.3148606e-07\n",
      "\n",
      "\tBest loss: 1.4077515602111816\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t674\t1.3356909\t9.817398e-07\n",
      "\n",
      "\tBest loss: 1.3356908559799194\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t580\t1.356987\t9.663327e-07\n",
      "\n",
      "\tBest loss: 1.3569869995117188\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.4080557\t0.00037618255\n",
      "\t\tFinal loss:\n",
      "\t\t1668\t1.3354975\t9.81882e-07\n",
      "\n",
      "\tBest loss: 1.335497498512268\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3252987\t5.217015e-06\n",
      "\t\t2000\t1.3165244\t7.968203e-06\n",
      "\t\t3000\t1.3048161\t9.410093e-06\n",
      "\t\t4000\t1.2928109\t8.390983e-06\n",
      "\t\t5000\t1.2839409\t5.199371e-06\n",
      "\t\t6000\t1.2798645\t1.5834135e-06\n",
      "\t\tFinal loss:\n",
      "\t\t6187\t1.279539\t9.3165727e-07\n",
      "\n",
      "\tBest loss: 1.2795389890670776\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3417205\t1.6881107e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1173\t1.341375\t9.775797e-07\n",
      "\n",
      "\tBest loss: 1.3413749933242798\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t659\t1.3716795\t8.690746e-07\n",
      "\n",
      "\tBest loss: 1.3716795444488525\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t896\t1.3064076\t9.124961e-07\n",
      "\n",
      "\tBest loss: 1.3064075708389282\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t892\t1.3276904\t9.876557e-07\n",
      "\n",
      "\tBest loss: 1.3276903629302979\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3798809\t2.0733796e-06\n",
      "\t\t2000\t1.3765321\t2.6846292e-06\n",
      "\t\t3000\t1.3730894\t2.1704527e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3884\t1.3709958\t9.564587e-07\n",
      "\n",
      "\tBest loss: 1.3709957599639893\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.4115522\t0.00014371771\n",
      "\t\tFinal loss:\n",
      "\t\t1703\t1.3777468\t8.652474e-07\n",
      "\n",
      "\tBest loss: 1.377746820449829\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3211671\t2.7429256e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1299\t1.3177174\t9.951306e-07\n",
      "\n",
      "\tBest loss: 1.317717432975769\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3051479\t2.557451e-06\n",
      "\t\t2000\t1.3014547\t3.0226904e-06\n",
      "\t\t3000\t1.297588\t2.6642203e-06\n",
      "\t\t4000\t1.2949547\t1.4729055e-06\n",
      "\t\tFinal loss:\n",
      "\t\t4123\t1.2947522\t9.2071036e-07\n",
      "\n",
      "\tBest loss: 1.2947522401809692\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t81\t1.3383012\t8.016751e-07\n",
      "\n",
      "\tBest loss: 1.338301181793213\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3126154\t1.2805194e-05\n",
      "\t\t2000\t1.3073002\t4.2857887e-06\n",
      "\t\t3000\t1.3005855\t5.9577455e-06\n",
      "\t\t4000\t1.2918509\t7.197624e-06\n",
      "\t\t5000\t1.2824721\t6.7855035e-06\n",
      "\t\t6000\t1.2748357\t4.675455e-06\n",
      "\t\t7000\t1.2707477\t1.7823936e-06\n",
      "\t\tFinal loss:\n",
      "\t\t7277\t1.2702187\t9.384933e-07\n",
      "\n",
      "\tBest loss: 1.2702187299728394\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t661\t1.4030544\t9.3460454e-07\n",
      "\n",
      "\tBest loss: 1.4030543565750122\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t42\t1.3847855\t8.608501e-08\n",
      "\n",
      "\tBest loss: 1.384785532951355\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3767558\t0.00021131444\n",
      "\t\tFinal loss:\n",
      "\t\t1458\t1.3487533\t9.722319e-07\n",
      "\n",
      "\tBest loss: 1.3487533330917358\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t73\t1.3712316\t6.9548696e-07\n",
      "\n",
      "\tBest loss: 1.3712315559387207\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t506\t1.3906409\t9.429472e-07\n",
      "\n",
      "\tBest loss: 1.3906408548355103\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3905182\t0.0003971156\n",
      "\t\t2000\t1.2661338\t1.6947369e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2047\t1.2660563\t8.47421e-07\n",
      "\n",
      "\tBest loss: 1.2660562992095947\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.2790747\t0.00020406554\n",
      "\t\tFinal loss:\n",
      "\t\t1640\t1.2412409\t9.604032e-07\n",
      "\n",
      "\tBest loss: 1.2412408590316772\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.6420105\t0.00087739417\n",
      "\t\t2000\t1.3583018\t1.491977e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2023\t1.3582612\t9.654261e-07\n",
      "\n",
      "\tBest loss: 1.358261227607727\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t626\t1.3410989\t9.777809e-07\n",
      "\n",
      "\tBest loss: 1.3410989046096802\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3114915\t0.00020738158\n",
      "\t\tFinal loss:\n",
      "\t\t1503\t1.2817537\t9.300475e-07\n",
      "\n",
      "\tBest loss: 1.281753659248352\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3036227\t1.3807948e-05\n",
      "\t\t2000\t1.288222\t8.883555e-06\n",
      "\t\t3000\t1.2815161\t2.3255466e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3350\t1.2807709\t9.3076113e-07\n",
      "\n",
      "\tBest loss: 1.2807708978652954\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t707\t1.3436556\t9.759204e-07\n",
      "\n",
      "\tBest loss: 1.3436555862426758\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t161\t1.2343967\t9.657282e-07\n",
      "\n",
      "\tBest loss: 1.2343966960906982\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3115325\t0.00012205457\n",
      "\t\tFinal loss:\n",
      "\t\t1420\t1.297728\t9.1859914e-07\n",
      "\n",
      "\tBest loss: 1.2977279424667358\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t221\t1.3063568\t9.1253156e-07\n",
      "\n",
      "\tBest loss: 1.3063567876815796\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t3.3154182\t0.0011865777\n",
      "\t\t2000\t1.4069086\t0.0004147574\n",
      "\t\t3000\t1.2190161\t1.4570706e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3464\t1.2158206\t9.804833e-07\n",
      "\n",
      "\tBest loss: 1.215820550918579\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3314013\t0.00010779057\n",
      "\t\tFinal loss:\n",
      "\t\t1515\t1.313233\t9.985287e-07\n",
      "\n",
      "\tBest loss: 1.3132330179214478\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t915\t1.3336791\t8.9383707e-07\n",
      "\n",
      "\tBest loss: 1.3336790800094604\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.2947711\t2.2188326e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1235\t1.2925211\t9.2229965e-07\n",
      "\n",
      "\tBest loss: 1.2925211191177368\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.2895703\t1.2941736e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1014\t1.2895505\t9.244242e-07\n",
      "\n",
      "\tBest loss: 1.289550542831421\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t443\t1.3085337\t8.199122e-07\n",
      "\n",
      "\tBest loss: 1.3085336685180664\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3988565\t7.840095e-06\n",
      "\t\t2000\t1.3851379\t1.1532325e-05\n",
      "\t\t3000\t1.3682579\t1.2284456e-05\n",
      "\t\t4000\t1.3532106\t9.073566e-06\n",
      "\t\t5000\t1.3444465\t3.990041e-06\n",
      "\t\tFinal loss:\n",
      "\t\t5743\t1.3420677\t9.77075e-07\n",
      "\n",
      "\tBest loss: 1.3420677185058594\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t49\t1.3832626\t0.0\n",
      "\n",
      "\tBest loss: 1.3832626342773438\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t635\t1.2887428\t8.3250336e-07\n",
      "\n",
      "\tBest loss: 1.2887427806854248\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t325\t1.4262444\t9.1940836e-07\n",
      "\n",
      "\tBest loss: 1.4262443780899048\n",
      "\n",
      "\n",
      "-----------------------\n",
      "ANN model:\n",
      "\n",
      "For outer fold 1 best generalization error is [1.21181834] for h = [8.] \n",
      "For outer fold 2 best generalization error is [1.25793946] for h = [7.] \n",
      "For outer fold 3 best generalization error is [1.27110481] for h = [2.] \n",
      "For outer fold 4 best generalization error is [1.30162418] for h = [3.] \n",
      "For outer fold 5 best generalization error is [1.23007703] for h = [10.] \n",
      "For outer fold 6 best generalization error is [1.18758047] for h = [5.] \n",
      "For outer fold 7 best generalization error is [1.27953899] for h = [5.] \n",
      "For outer fold 8 best generalization error is [1.27021873] for h = [5.] \n",
      "For outer fold 9 best generalization error is [1.2343967] for h = [8.] \n",
      "For outer fold 10 best generalization error is [1.21582055] for h = [1.] \n"
     ]
    }
   ],
   "source": [
    "# ANN\n",
    "\n",
    "# Create crossvalidation partition for evaluation\n",
    "# Set the number of folds for cross-validation\n",
    "K = 10\n",
    "# Set the number of folds for inner cross-validation\n",
    "K_i = 10\n",
    "# Create K-fold cross-validation partitions for evaluation\n",
    "CV = model_selection.KFold(K, shuffle=True)\n",
    "#CV = model_selection.KFold(K, shuffle=False)\n",
    "\n",
    "# Initialize variables for storing results\n",
    "mu = np.empty((K, M-1))\n",
    "sigma = np.empty((K, M-1))\n",
    "error_i = np.empty((K_i, 1))\n",
    "minerror = np.empty((K, 1))\n",
    "min_ind = np.empty((K, 1))\n",
    "\n",
    "# Loop over the K folds for cross-validation\n",
    "k_o = 1\n",
    "for train_index, test_index in CV.split(X,y):\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "\n",
    "    # Parameters for neural network classifier\n",
    "    # K-fold crossvalidation\n",
    "    CV = model_selection.KFold(K_i, shuffle=True)\n",
    "    \n",
    "    #print('Training model of type:\\n\\n{}\\n'.format(str(model())))\n",
    "    # Initialize a list for storing generalizaition error in each loop\n",
    "    errors = np.empty((K_i, 1))\n",
    "\n",
    "    # Loop over the K_i folds for the inner cross-validation\n",
    "    for (k, (train_index, test_index)) in enumerate(CV.split(X_train,y_train)): \n",
    "        #print('\\nCrossvalidation fold: {0}/{1}'.format(k+1,K_i))  \n",
    "        \n",
    "        # Set the number of hidden units for the neural network\n",
    "        n_hidden_units = k + 1\n",
    "\n",
    "        # Set the number of networks trained in each k-fold\n",
    "        n_replicates = 1\n",
    "\n",
    "        # Set the maximum number of iterations\n",
    "        max_iter = 10000\n",
    "        \n",
    "        # Define the model using a lambda function\n",
    "        model = lambda: torch.nn.Sequential(\n",
    "                            torch.nn.Linear(M, n_hidden_units), #M features to n_hidden_units\n",
    "                            torch.nn.Tanh(),   # 1st transfer function,\n",
    "                            torch.nn.Linear(n_hidden_units, 1), # n_hidden_units to 1 output neuron\n",
    "                            # no final tranfer function, i.e. \"linear output\"\n",
    "                            )\n",
    "        # Set the loss function\n",
    "        loss_fn = torch.nn.MSELoss() # notice how this is now a mean-squared-error loss\n",
    "        \n",
    "        # Extract training and test set for current CV fold, convert to tensors\n",
    "        X_train_i = torch.Tensor(X_train[train_index,:])\n",
    "        y_train_i = torch.Tensor(y_train[train_index])\n",
    "        X_test_i = torch.Tensor(X_train[test_index,:])\n",
    "        y_test_i = torch.Tensor(y_train[test_index])\n",
    "        \n",
    "        # Train the net on training data\n",
    "        net, final_loss, learning_curve = train_neural_net(model,\n",
    "                                                           loss_fn,\n",
    "                                                           X=X_train_i,\n",
    "                                                           y=y_train_i,\n",
    "                                                           n_replicates=n_replicates,\n",
    "                                                           max_iter=max_iter)\n",
    "        \n",
    "        # Print the best loss for the current cross-validation fold\n",
    "        print('\\n\\tBest loss: {}\\n'.format(final_loss))\n",
    "        \n",
    "        # Determine estimated class labels for test set\n",
    "        y_test_est = net(X_test_i)\n",
    "        \n",
    "        # Store error rate for current CV fold\n",
    "        errors[k] = final_loss\n",
    "    \n",
    "    # Store the minimum error rate and its index for the current outer fold\n",
    "    minerror[k_o-1]=min(errors)\n",
    "    min_ind[k_o-1] = np.argmin(errors)\n",
    "    \n",
    "    # Increment the outer fold counter\n",
    "    k_o += 1\n",
    "\n",
    "\n",
    "print('\\n-----------------------')\n",
    "print('ANN model:\\n')  \n",
    "\n",
    "for ii in range(10):\n",
    "    print('For outer fold {} best generalization error is {} for h = {} '.format(ii+1,minerror[ii],min_ind[ii]+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\Jakob Højgaard\\\\OneDrive - Danmarks Tekniske Universitet\\\\DTU\\\\8. Semester\\\\02450 Introduction to Machine Learning and Data Mining\\\\project_1\\\\02450intro_machine_learning\\\\Project 2\\\\Scripts', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\python310.zip', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\DLLs', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\lib', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10', '', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\lib\\\\site-packages', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\lib\\\\site-packages\\\\win32', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\lib\\\\site-packages\\\\Pythonwin', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Jakob Højgaard\\\\.ipython', '../Tools']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Correlated t-test for linear regression and baseline\n",
      "p-value 9.575570832940812e-05 and confidence interval (0.2895742511212311, 0.5894246287810803)\n",
      " \n",
      "Correlated t-test for ANN and baseline\n",
      "p-value 0.00046336315638406233 and confidence interval (0.20382667409896718, 0.5026349575427906)\n",
      " \n",
      "Correlated t-test for linear regression and ANN\n",
      "p-value 0.005085456088433801 and confidence interval (-0.13931764713285957, -0.03321960112769409)\n"
     ]
    }
   ],
   "source": [
    "# Statistical test for method setup II\n",
    "\n",
    "# MSE are saved in variable:\n",
    "    # errors_lin_reg for linear regression\n",
    "    # error_o for baseline\n",
    "    # minerror for ANN\n",
    "\n",
    "# Generalization error differences\n",
    "r_lin_base = errors_lin_reg - error_o # Calculate difference between linear regression and baseline errors\n",
    "r_lin_ANN = minerror - errors_lin_reg # Calculate difference between ANN and linear regression errors\n",
    "r_ANN_base = minerror - error_o # Calculate difference between ANN and baseline errors\n",
    "\n",
    "# significance level\n",
    "alpha = 0.05\n",
    "\n",
    "rho = 1/K\n",
    "\n",
    "print(' ')\n",
    "# Test lin reg vs baseline\n",
    "print('Correlated t-test for linear regression and baseline')\n",
    "p_val, conf_int = correlated_ttest(r_lin_base, rho, alpha) # Perform correlated t-test between linear regression and baseline errors\n",
    "print('p-value {} and confidence interval {}'.format(p_val, conf_int))\n",
    "print(' ')\n",
    "\n",
    "# Test ANN vs baseline\n",
    "print('Correlated t-test for ANN and baseline')\n",
    "p_val, conf_int = correlated_ttest(r_ANN_base, rho, alpha) # Perform correlated t-test between ANN and baseline errors\n",
    "print('p-value {} and confidence interval {}'.format(p_val, conf_int))\n",
    "print(' ')\n",
    "\n",
    "# Test lin reg vs ANN\n",
    "print('Correlated t-test for linear regression and ANN')\n",
    "p_val, conf_int = correlated_ttest(r_lin_ANN, rho, alpha) # Perform correlated t-test between linear regression and ANN errors\n",
    "print('p-value {} and confidence interval {}'.format(p_val, conf_int))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dd5f8e12b52256db706a7757d4ff925217ebc162bb7ba53c9abf8d847d6b988"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
