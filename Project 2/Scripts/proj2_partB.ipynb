{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import (figure, semilogx, loglog, xlabel, ylabel, legend, \n",
    "                           title, subplot, show, grid)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../Tools')\n",
    "from toolbox_02450 import rlr_validate, correlated_ttest, train_neural_net, draw_neural_net\n",
    "\n",
    "from data_preprocessing import *\n",
    "dfjoint, dfRec, dfClas = dataPreprocess()\n",
    "\n",
    "colNamesMeans, colNamesStd, colNamesExt, colNamesOther = getSpecificColNames()\n",
    "mean = dfjoint.loc[:,colNamesMeans]\n",
    "sde = dfjoint.loc[:,colNamesStd]\n",
    "worst = dfjoint.loc[:,colNamesExt]\n",
    "colNames = colNamesMeans + colNamesStd + colNamesExt\n",
    "\n",
    "time_discretized = []\n",
    "for i in range(len(dfRec.iloc[:,0])):\n",
    "    if dfRec['time'].iloc[i] <= 12:\n",
    "        time_discretized.append('<1 years')\n",
    "    elif dfRec['time'].iloc[i] <= 36:\n",
    "        time_discretized.append('1-3 years')\n",
    "    elif dfRec['time'].iloc[i] <= 72:\n",
    "        time_discretized.append('>3-6 years')\n",
    "    else:\n",
    "        time_discretized.append('6+ years')\n",
    "\n",
    "dfRec['time_discretized'] = time_discretized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pylab import (figure, semilogx, loglog, xlabel, ylabel, legend, \n",
    "                           title, subplot, show, grid)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "import sys\n",
    "sys.path.append('../Tools')\n",
    "from toolbox_02450 import rlr_validate, correlated_ttest, train_neural_net, draw_neural_net\n",
    "\n",
    "from data_preprocessing import *\n",
    "dfjoint, dfRec, dfClas = dataPreprocess()\n",
    "\n",
    "colNamesMeans, colNamesStd, colNamesExt, colNamesOther = getSpecificColNames()\n",
    "mean = dfjoint.loc[:,colNamesMeans]\n",
    "sde = dfjoint.loc[:,colNamesStd]\n",
    "worst = dfjoint.loc[:,colNamesExt]\n",
    "colNames = colNamesMeans + colNamesStd + colNamesExt\n",
    "\n",
    "time_discretized = []\n",
    "for i in range(len(dfRec.iloc[:,0])):\n",
    "    if dfRec['time'].iloc[i] <= 12:\n",
    "        time_discretized.append('<1 years')\n",
    "    elif dfRec['time'].iloc[i] <= 36:\n",
    "        time_discretized.append('1-3 years')\n",
    "    elif dfRec['time'].iloc[i] <= 72:\n",
    "        time_discretized.append('>3-6 years')\n",
    "    else:\n",
    "        time_discretized.append('6+ years')\n",
    "\n",
    "dfRec['time_discretized'] = time_discretized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "attributeNames = [u'Offset'] + colNames\n",
    "X = dfRec.loc[:, colNames]\n",
    "\n",
    "classLabels = dfRec.loc[:,\"time_discretized\"]\n",
    "classNames = sorted(set(dfRec.loc[:,\"time_discretized\"]))\n",
    "classDict = dict(zip(classNames, range(4)))\n",
    "                 \n",
    "y = np.asarray([classDict[value] for value in classLabels])\n",
    "\n",
    "N = X.shape[0]\n",
    "M = len(attributeNames)\n",
    "# M = M + 1\n",
    "C = len(classNames)\n",
    "\n",
    "X = np.concatenate((np.ones((X.shape[0],1)),X),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2 level cross validation, linear regression model\n",
    "\n",
    "# Crossvalidation\n",
    "# Create crossvalidation partition for evaluation\n",
    "K = 10\n",
    "CV = model_selection.KFold(K, shuffle=True)\n",
    "#CV = model_selection.KFold(K, shuffle=False)\n",
    "\n",
    "# Values of lambda\n",
    "lambdas = np.power(10.,range(-5,9))\n",
    "\n",
    "# Initialize variables\n",
    "#T = len(lambdas)\n",
    "Error_train = np.empty((K,1))\n",
    "Error_test = np.empty((K,1))\n",
    "Error_train_rlr = np.empty((K,1))\n",
    "Error_test_rlr = np.empty((K,1))\n",
    "Error_train_nofeatures = np.empty((K,1))\n",
    "Error_test_nofeatures = np.empty((K,1))\n",
    "w_rlr = np.empty((M,K))\n",
    "mu = np.empty((K, M-1))\n",
    "sigma = np.empty((K, M-1))\n",
    "w_noreg = np.empty((M,K))\n",
    "errors_lin_reg = np.empty((K,1))\n",
    "\n",
    "k=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "------------------------------------\n",
      "Linear regression model:\n",
      " \n",
      "Outer fold 1\n",
      "has optimal lambda: 100000000.0\n",
      "and optimal generalization error: 1.3242008403009602\n",
      "Outer fold 2\n",
      "has optimal lambda: 1000.0\n",
      "and optimal generalization error: 1.3890844917004554\n",
      "Outer fold 3\n",
      "has optimal lambda: 10000.0\n",
      "and optimal generalization error: 1.2749842939922542\n",
      "Outer fold 4\n",
      "has optimal lambda: 100000000.0\n",
      "and optimal generalization error: 1.4338812908882228\n",
      "Outer fold 5\n",
      "has optimal lambda: 100000000.0\n",
      "and optimal generalization error: 1.2917535265096134\n"
     ]
    }
   ],
   "source": [
    "print(' ')\n",
    "print('------------------------------------')\n",
    "print('Linear regression model:')\n",
    "print(' ')\n",
    "k=0\n",
    "for train_index, test_index in CV.split(X,y):\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "    internal_cross_validation = 10    \n",
    "    \n",
    "    opt_val_err, opt_lambda, mean_w_vs_lambda, train_err_vs_lambda, test_err_vs_lambda = rlr_validate(X_train, y_train, lambdas, internal_cross_validation)\n",
    "\n",
    "    # Standardize outer fold based on training set, and save the mean and standard\n",
    "    # deviations since they're part of the model (they would be needed for\n",
    "    # making new predictions) - for brevity we won't always store these in the scripts\n",
    "    mu[k, :] = np.mean(X_train[:, 1:], 0)\n",
    "    sigma[k, :] = np.std(X_train[:, 1:], 0)\n",
    "    \n",
    "    X_train[:, 1:] = (X_train[:, 1:] - mu[k, :] ) / sigma[k, :] \n",
    "    X_test[:, 1:] = (X_test[:, 1:] - mu[k, :] ) / sigma[k, :]\n",
    "    \n",
    "    # Save generalization error for later statistics\n",
    "    errors_lin_reg[k] = opt_val_err\n",
    "    \n",
    "    k+=1\n",
    "    \n",
    "    print('Outer fold {0}'.format(k))\n",
    "    print('has optimal lambda: {0}'.format(opt_lambda))\n",
    "    print('and optimal generalization error: {0}'.format(opt_val_err))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "------------------------------------\n",
      "Baseline model:\n",
      " \n",
      "Outer fold 1 generalization error: [0.89246364]\n",
      "Outer fold 2 generalization error: [0.63545455]\n",
      "Outer fold 3 generalization error: [0.91340909]\n",
      "Outer fold 4 generalization error: [0.85687273]\n",
      "Outer fold 5 generalization error: [0.89707783]\n",
      "Outer fold 6 generalization error: [0.8124]\n",
      "Outer fold 7 generalization error: [0.82547166]\n",
      "Outer fold 8 generalization error: [0.91293333]\n",
      "Outer fold 9 generalization error: [1.06334495]\n",
      "Outer fold 10 generalization error: [0.6436713]\n"
     ]
    }
   ],
   "source": [
    "#%% 2 level Cross validation, baseline\n",
    "\n",
    "# Create crossvalidation partition for evaluation\n",
    "K = 10\n",
    "K_i = 10\n",
    "CV = model_selection.KFold(K, shuffle=True)\n",
    "#CV = model_selection.KFold(K, shuffle=False)\n",
    "\n",
    "# Initialize variables\n",
    "mu = np.empty((K, M-1))\n",
    "sigma = np.empty((K, M-1))\n",
    "error_i = np.empty((K_i,1))\n",
    "error_o = np.empty((K,1))\n",
    "\n",
    "\n",
    "print(' ')\n",
    "print('------------------------------------')\n",
    "print('Baseline model:')\n",
    "print(' ')\n",
    "k=0\n",
    "for train_index, test_index in CV.split(X,y):\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    \n",
    "    CV_i = model_selection.KFold(K_i, shuffle=True)\n",
    "    \n",
    "    k_i = 0\n",
    "    for train_index, test_index in CV_i.split(X_train,y_train):\n",
    "        \n",
    "        # extract training and test set for current inner CV fold\n",
    "        X_train_i = X_train[train_index]\n",
    "        y_train_i = y_train[train_index]\n",
    "        X_test_i = X_train[test_index]\n",
    "        y_test_i = y_train[test_index]\n",
    "        \n",
    "        mean_i = np.mean(y_train_i)\n",
    "        error_i[k_i]=(1/len(y_test_i))*np.sum((y_test_i-mean_i)**2)\n",
    "        #rykke error her op og find 10 inder errors\n",
    "        #Error_i[k_i] = (1/len(y_test_i)*\n",
    "        \n",
    "        k_i+=1\n",
    "    \n",
    "    error_o[k] = min(error_i)\n",
    "        \n",
    "    # Standardize outer fold based on training set, and save the mean and standard\n",
    "    # deviations since they're part of the model (they would be needed for\n",
    "    # making new predictions) - for brevity we won't always store these in the scripts\n",
    "    mu[k, :] = np.mean(X_train[:, 1:], 0)\n",
    "    sigma[k, :] = np.std(X_train[:, 1:], 0)\n",
    "    \n",
    "    X_train[:, 1:] = (X_train[:, 1:] - mu[k, :] ) / sigma[k, :] \n",
    "    X_test[:, 1:] = (X_test[:, 1:] - mu[k, :] ) / sigma[k, :] \n",
    "    \n",
    "    print('Outer fold {} generalization error: {}'.format(k+1,error_o[k]))\n",
    "\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\py10\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([99])) that is different to the input size (torch.Size([99, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tFinal loss:\n",
      "\t\t167\t1.3096805\t9.1021576e-07\n",
      "\n",
      "\tBest loss: 1.309680461883545\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\py10\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t1.3661934\t3.141225e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1073\t1.3660035\t9.599543e-07\n",
      "\n",
      "\tBest loss: 1.3660035133361816\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t942\t1.2900808\t9.240443e-07\n",
      "\n",
      "\tBest loss: 1.2900807857513428\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t193\t1.2739145\t7.486167e-07\n",
      "\n",
      "\tBest loss: 1.2739144563674927\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t439\t1.3139387\t9.979924e-07\n",
      "\n",
      "\tBest loss: 1.3139387369155884\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t60\t1.3500383\t9.713084e-07\n",
      "\n",
      "\tBest loss: 1.3500382900238037\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.2933842\t2.9493835e-06\n",
      "\t\t2000\t1.2886734\t4.255232e-06\n",
      "\t\t3000\t1.2826508\t4.925784e-06\n",
      "\t\t4000\t1.276919\t3.734265e-06\n",
      "\t\t5000\t1.2731925\t2.0598604e-06\n",
      "\t\tFinal loss:\n",
      "\t\t5414\t1.2723883\t9.3689306e-07\n",
      "\n",
      "\tBest loss: 1.2723883390426636\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.267461\t5.5491328e-06\n",
      "\t\t2000\t1.2598943\t5.6770773e-06\n",
      "\t\t3000\t1.2542394\t3.0414333e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3764\t1.2522676\t9.519465e-07\n",
      "\n",
      "\tBest loss: 1.252267599105835\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t480\t1.3139428\t9.979893e-07\n",
      "\n",
      "\tBest loss: 1.313942790031433\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t48\t1.2651147\t2.8268408e-07\n",
      "\n",
      "\tBest loss: 1.265114665031433\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t79\t1.2892594\t5.5478e-07\n",
      "\n",
      "\tBest loss: 1.289259433746338\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t647\t1.2844677\t9.2808233e-07\n",
      "\n",
      "\tBest loss: 1.2844676971435547\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t404\t1.2571397\t9.482572e-07\n",
      "\n",
      "\tBest loss: 1.2571396827697754\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t808\t1.3275737\t8.9794776e-07\n",
      "\n",
      "\tBest loss: 1.3275736570358276\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t859\t1.2641509\t9.4299804e-07\n",
      "\n",
      "\tBest loss: 1.264150857925415\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.2909716\t2.216173e-06\n",
      "\t\t2000\t1.2873086\t3.2411124e-06\n",
      "\t\t3000\t1.2826068\t3.9965316e-06\n",
      "\t\t4000\t1.2781067\t2.984638e-06\n",
      "\t\t5000\t1.275163\t1.869707e-06\n",
      "\t\tFinal loss:\n",
      "\t\t5178\t1.2748507\t9.3508345e-07\n",
      "\n",
      "\tBest loss: 1.2748507261276245\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t317\t1.2339244\t9.660979e-07\n",
      "\n",
      "\tBest loss: 1.233924388885498\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3090998\t1.4751832e-05\n",
      "\t\t2000\t1.2945509\t6.7221918e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2909\t1.2904747\t8.313861e-07\n",
      "\n",
      "\tBest loss: 1.2904746532440186\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t417\t1.2571365\t9.4825964e-07\n",
      "\n",
      "\tBest loss: 1.2571364641189575\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t91\t1.2811007\t9.305215e-07\n",
      "\n",
      "\tBest loss: 1.2811007499694824\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3702296\t0.00014048445\n",
      "\t\tFinal loss:\n",
      "\t\t1672\t1.3386071\t9.79601e-07\n",
      "\n",
      "\tBest loss: 1.3386070728302002\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.5249194\t0.00058994483\n",
      "\t\tFinal loss:\n",
      "\t\t1995\t1.3420721\t9.770719e-07\n",
      "\n",
      "\tBest loss: 1.3420721292495728\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t711\t1.3644644\t9.610371e-07\n",
      "\n",
      "\tBest loss: 1.3644644021987915\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t190\t1.3179168\t9.045274e-07\n",
      "\n",
      "\tBest loss: 1.317916750907898\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t498\t1.3444417\t8.8668173e-07\n",
      "\n",
      "\tBest loss: 1.3444416522979736\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t266\t1.3436211\t9.759455e-07\n",
      "\n",
      "\tBest loss: 1.3436211347579956\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t157\t1.3376138\t8.912078e-07\n",
      "\n",
      "\tBest loss: 1.337613821029663\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t69\t1.4075001\t4.234786e-07\n",
      "\n",
      "\tBest loss: 1.407500147819519\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t715\t1.3600585\t9.641503e-07\n",
      "\n",
      "\tBest loss: 1.3600585460662842\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t150\t1.2676238\t9.404145e-07\n",
      "\n",
      "\tBest loss: 1.267623782157898\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.8123193\t0.00080952194\n",
      "\t\t2000\t1.3391124\t3.0711344e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2555\t1.3318987\t9.845351e-07\n",
      "\n",
      "\tBest loss: 1.3318986892700195\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.850045\t0.0009896569\n",
      "\t\t2000\t1.377718\t1.1421384e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2313\t1.3757743\t9.531367e-07\n",
      "\n",
      "\tBest loss: 1.3757742643356323\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3861943\t1.7199476e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1043\t1.3861135\t9.460271e-07\n",
      "\n",
      "\tBest loss: 1.3861135244369507\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3290269\t3.1393736e-06\n",
      "\t\t2000\t1.3236274\t4.9534174e-06\n",
      "\t\t3000\t1.3163619\t5.976902e-06\n",
      "\t\t4000\t1.3088251\t5.3737585e-06\n",
      "\t\t5000\t1.3031958\t3.2016e-06\n",
      "\t\tFinal loss:\n",
      "\t\t5893\t1.300754\t9.1646217e-07\n",
      "\n",
      "\tBest loss: 1.3007539510726929\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t79\t1.3339009\t9.830571e-07\n",
      "\n",
      "\tBest loss: 1.3339009284973145\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t830\t1.3636713\t9.61596e-07\n",
      "\n",
      "\tBest loss: 1.3636713027954102\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3715112\t1.6340355e-05\n",
      "\t\t2000\t1.348652\t1.564504e-05\n",
      "\t\t3000\t1.3313866\t1.0475801e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3873\t1.3141499\t9.978321e-07\n",
      "\n",
      "\tBest loss: 1.3141498565673828\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.4262217\t6.769844e-05\n",
      "\t\t2000\t1.3724564\t1.5286834e-05\n",
      "\t\t3000\t1.3624111\t3.4999382e-06\n",
      "\t\t4000\t1.3591391\t1.4910578e-06\n",
      "\t\tFinal loss:\n",
      "\t\t4187\t1.3588016\t9.650422e-07\n",
      "\n",
      "\tBest loss: 1.3588016033172607\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t54\t1.2959309\t6.439113e-07\n",
      "\n",
      "\tBest loss: 1.2959308624267578\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t373\t1.2844235\t9.281143e-07\n",
      "\n",
      "\tBest loss: 1.2844234704971313\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t3.5349298\t0.0011559763\n",
      "\t\t2000\t1.5225301\t0.00041894737\n",
      "\t\t3000\t1.3118\t1.6538903e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3516\t1.3077136\t9.115848e-07\n",
      "\n",
      "\tBest loss: 1.3077136278152466\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3089997\t5.828382e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1130\t1.3085036\t9.110344e-07\n",
      "\n",
      "\tBest loss: 1.3085036277770996\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\py10\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([101])) that is different to the input size (torch.Size([101, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t1.3125222\t3.3604986e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1072\t1.3123243\t9.992201e-07\n",
      "\n",
      "\tBest loss: 1.3123242855072021\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.333724\t1.4300904e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1025\t1.3336796\t9.832204e-07\n",
      "\n",
      "\tBest loss: 1.3336795568466187\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t709\t1.3414763\t9.775058e-07\n",
      "\n",
      "\tBest loss: 1.341476321220398\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t803\t1.2760105\t9.342335e-07\n",
      "\n",
      "\tBest loss: 1.276010513305664\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3169731\t0.000315264\n",
      "\t\tFinal loss:\n",
      "\t\t1531\t1.2727193\t9.366494e-07\n",
      "\n",
      "\tBest loss: 1.2727192640304565\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t873\t1.2958142\t7.359648e-07\n",
      "\n",
      "\tBest loss: 1.2958141565322876\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t75\t1.3450928\t3.545012e-07\n",
      "\n",
      "\tBest loss: 1.3450927734375\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3219547\t9.91932e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1550\t1.3187706\t9.943358e-07\n",
      "\n",
      "\tBest loss: 1.3187706470489502\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t2.9235632\t0.001130725\n",
      "\t\t2000\t1.4245045\t0.0002711487\n",
      "\t\t3000\t1.3124676\t5.5405e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3281\t1.3113952\t9.999281e-07\n",
      "\n",
      "\tBest loss: 1.3113951683044434\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3070303\t0.00031082536\n",
      "\t\tFinal loss:\n",
      "\t\t1759\t1.2412349\t9.604079e-07\n",
      "\n",
      "\tBest loss: 1.2412348985671997\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t223\t1.3063602\t9.125292e-07\n",
      "\n",
      "\tBest loss: 1.3063602447509766\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t684\t1.2542448\t9.5044584e-07\n",
      "\n",
      "\tBest loss: 1.2542448043823242\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3507879\t1.6591039e-05\n",
      "\t\t2000\t1.3393128\t2.3141974e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2256\t1.3387042\t9.7953e-07\n",
      "\n",
      "\tBest loss: 1.338704228401184\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3458282\t3.1887582e-06\n",
      "\t\t2000\t1.3425742\t1.5094546e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2294\t1.3420193\t9.771103e-07\n",
      "\n",
      "\tBest loss: 1.3420193195343018\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t773\t1.2644477\t9.4277664e-07\n",
      "\n",
      "\tBest loss: 1.2644476890563965\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.2882242\t5.82984e-06\n",
      "\t\t2000\t1.2804279\t5.865337e-06\n",
      "\t\t3000\t1.2742063\t3.4615498e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3851\t1.2716556\t9.374329e-07\n",
      "\n",
      "\tBest loss: 1.271655559539795\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t789\t1.3840612\t9.474299e-07\n",
      "\n",
      "\tBest loss: 1.3840612173080444\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3072048\t1.03048205e-05\n",
      "\t\t2000\t1.2964908\t5.424882e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2898\t1.2929674\t9.219813e-07\n",
      "\n",
      "\tBest loss: 1.292967438697815\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.329328\t5.3623586e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1458\t1.3201424\t9.933026e-07\n",
      "\n",
      "\tBest loss: 1.320142388343811\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t670\t1.2864611\t9.2664425e-07\n",
      "\n",
      "\tBest loss: 1.2864611148834229\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.2517878\t4.1042975e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1327\t1.2470326\t9.559427e-07\n",
      "\n",
      "\tBest loss: 1.247032642364502\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t802\t1.2817168\t9.3007424e-07\n",
      "\n",
      "\tBest loss: 1.2817168235778809\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t752\t1.3040553\t9.1414205e-07\n",
      "\n",
      "\tBest loss: 1.3040553331375122\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t574\t1.2823892\t8.3662803e-07\n",
      "\n",
      "\tBest loss: 1.2823891639709473\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3368288\t9.54144e-06\n",
      "\t\t2000\t1.3208857\t1.3717738e-05\n",
      "\t\t3000\t1.3027185\t1.3268497e-05\n",
      "\t\t4000\t1.288349\t8.420043e-06\n",
      "\t\t5000\t1.2813067\t2.8841473e-06\n",
      "\t\tFinal loss:\n",
      "\t\t5513\t1.280047\t9.3128745e-07\n",
      "\n",
      "\tBest loss: 1.280047059059143\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t54\t1.3576328\t8.780665e-07\n",
      "\n",
      "\tBest loss: 1.3576327562332153\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3148825\t2.4659334e-05\n",
      "\t\t2000\t1.2984539\t2.019786e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2773\t1.2969604\t9.1914285e-07\n",
      "\n",
      "\tBest loss: 1.2969603538513184\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t117\t1.2865424\t9.265857e-07\n",
      "\n",
      "\tBest loss: 1.2865424156188965\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3217634\t5.952477e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1140\t1.3212223\t9.924908e-07\n",
      "\n",
      "\tBest loss: 1.3212223052978516\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t473\t1.2675406\t9.4047624e-07\n",
      "\n",
      "\tBest loss: 1.2675405740737915\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t573\t1.3212965\t9.92435e-07\n",
      "\n",
      "\tBest loss: 1.3212964534759521\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t22\t1.3329494\t8.9432724e-08\n",
      "\n",
      "\tBest loss: 1.3329493999481201\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t63\t1.3124275\t9.991415e-07\n",
      "\n",
      "\tBest loss: 1.3124275207519531\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.2972841\t2.5729532e-06\n",
      "\t\t2000\t1.2931945\t4.148174e-06\n",
      "\t\t3000\t1.2871747\t5.371533e-06\n",
      "\t\t4000\t1.2798846\t5.867827e-06\n",
      "\t\t5000\t1.2729137\t4.869814e-06\n",
      "\t\t6000\t1.2681922\t2.537977e-06\n",
      "\t\tFinal loss:\n",
      "\t\t6663\t1.2666801\t8.4700366e-07\n",
      "\n",
      "\tBest loss: 1.266680121421814\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t394\t1.2459452\t7.6542176e-07\n",
      "\n",
      "\tBest loss: 1.2459452152252197\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t496\t1.2789276\t8.3889245e-07\n",
      "\n",
      "\tBest loss: 1.2789275646209717\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t639\t1.2731212\t9.363537e-07\n",
      "\n",
      "\tBest loss: 1.2731212377548218\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3270388\t3.0542465e-06\n",
      "\t\t2000\t1.3220719\t4.5084066e-06\n",
      "\t\t3000\t1.3155131\t5.3464387e-06\n",
      "\t\t4000\t1.3089197\t4.4626336e-06\n",
      "\t\t5000\t1.3042344\t2.5592421e-06\n",
      "\t\tFinal loss:\n",
      "\t\t5727\t1.3025566\t9.151938e-07\n",
      "\n",
      "\tBest loss: 1.3025566339492798\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t229\t1.3819329\t9.4888907e-07\n",
      "\n",
      "\tBest loss: 1.3819328546524048\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t120\t1.301108\t6.413491e-07\n",
      "\n",
      "\tBest loss: 1.3011080026626587\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t935\t1.2893608\t9.245603e-07\n",
      "\n",
      "\tBest loss: 1.289360761642456\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t757\t1.3085585\t9.109962e-07\n",
      "\n",
      "\tBest loss: 1.308558464050293\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3534726\t3.7872803e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1887\t1.3507007\t9.708301e-07\n",
      "\n",
      "\tBest loss: 1.3507007360458374\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t965\t1.3040651\t9.1413517e-07\n",
      "\n",
      "\tBest loss: 1.3040651082992554\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3091626\t1.8211499e-06\n",
      "\t\t2000\t1.3066216\t2.1896292e-06\n",
      "\t\t3000\t1.3039719\t1.7369796e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3711\t1.3025811\t9.1517666e-07\n",
      "\n",
      "\tBest loss: 1.3025810718536377\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.366473\t1.919247e-06\n",
      "\t\t2000\t1.3636484\t1.8358035e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2761\t1.3619984\t9.627771e-07\n",
      "\n",
      "\tBest loss: 1.361998438835144\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t362\t1.3353857\t9.819641e-07\n",
      "\n",
      "\tBest loss: 1.3353856801986694\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t578\t1.3010099\t9.1628186e-07\n",
      "\n",
      "\tBest loss: 1.3010098934173584\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t320\t1.3504412\t9.710167e-07\n",
      "\n",
      "\tBest loss: 1.3504412174224854\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t685\t1.2811725\t9.3046935e-07\n",
      "\n",
      "\tBest loss: 1.281172513961792\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.4180007\t0.0005630263\n",
      "\t\tFinal loss:\n",
      "\t\t1849\t1.2850981\t9.2762707e-07\n",
      "\n",
      "\tBest loss: 1.2850980758666992\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t607\t1.3320947\t9.843901e-07\n",
      "\n",
      "\tBest loss: 1.332094669342041\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.2817307\t1.3950956e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1009\t1.2817172\t9.3007395e-07\n",
      "\n",
      "\tBest loss: 1.2817171812057495\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t893\t1.3040522\t9.141442e-07\n",
      "\n",
      "\tBest loss: 1.3040522336959839\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3329848\t2.0389703e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1212\t1.3310773\t9.851425e-07\n",
      "\n",
      "\tBest loss: 1.3310773372650146\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.2762942\t2.8020722e-06\n",
      "\t\t2000\t1.2721293\t3.5609091e-06\n",
      "\t\t3000\t1.267439\t3.3859774e-06\n",
      "\t\t4000\t1.2638408\t2.2637475e-06\n",
      "\t\tFinal loss:\n",
      "\t\t4542\t1.2627206\t9.4406613e-07\n",
      "\n",
      "\tBest loss: 1.2627205848693848\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t56\t1.2696797\t0.0\n",
      "\n",
      "\tBest loss: 1.2696796655654907\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t825\t1.2991524\t9.17592e-07\n",
      "\n",
      "\tBest loss: 1.2991523742675781\n",
      "\n",
      " \n",
      "------------------------------------\n",
      "ANN model:\n",
      " \n",
      "For outer fold 1 best generalization error is [1.2522676] for h = [8.] \n",
      "For outer fold 2 best generalization error is [1.23392439] for h = [7.] \n",
      "For outer fold 3 best generalization error is [1.26762378] for h = [10.] \n",
      "For outer fold 4 best generalization error is [1.28442347] for h = [10.] \n",
      "For outer fold 5 best generalization error is [1.27271926] for h = [7.] \n",
      "For outer fold 6 best generalization error is [1.2412349] for h = [2.] \n",
      "For outer fold 7 best generalization error is [1.24703264] for h = [3.] \n",
      "For outer fold 8 best generalization error is [1.24594522] for h = [7.] \n",
      "For outer fold 9 best generalization error is [1.28936076] for h = [3.] \n",
      "For outer fold 10 best generalization error is [1.26272058] for h = [8.] \n"
     ]
    }
   ],
   "source": [
    "# ANN\n",
    "\n",
    "# Create crossvalidation partition for evaluation\n",
    "K = 10\n",
    "K_i = 10\n",
    "CV = model_selection.KFold(K, shuffle=True)\n",
    "#CV = model_selection.KFold(K, shuffle=False)\n",
    "\n",
    "# Initialize variables\n",
    "mu = np.empty((K, M-1))\n",
    "sigma = np.empty((K, M-1))\n",
    "error_i = np.empty((K_i,1))\n",
    "\n",
    "minerror=np.empty((K,1))\n",
    "min_ind=np.empty((K,1))\n",
    "\n",
    "\n",
    "k_o=1\n",
    "for train_index, test_index in CV.split(X,y):\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "\n",
    "    # Parameters for neural network classifier\n",
    "    # K-fold crossvalidation\n",
    "    CV = model_selection.KFold(K_i, shuffle=True)\n",
    "    \n",
    "    #print('Training model of type:\\n\\n{}\\n'.format(str(model())))\n",
    "    errors = np.empty((K,1)) # make a list for storing generalizaition error in each loop\n",
    "    for (k, (train_index, test_index)) in enumerate(CV.split(X_train,y_train)): \n",
    "        #print('\\nCrossvalidation fold: {0}/{1}'.format(k+1,K_i))  \n",
    "        n_hidden_units = k+1      # number of hidden units\n",
    "        n_replicates = 1        # number of networks trained in each k-fold\n",
    "        max_iter = 10000\n",
    "        \n",
    "        # Define the model\n",
    "        model = lambda: torch.nn.Sequential(\n",
    "                            torch.nn.Linear(M, n_hidden_units), #M features to n_hidden_units\n",
    "                            torch.nn.Tanh(),   # 1st transfer function,\n",
    "                            torch.nn.Linear(n_hidden_units, 1), # n_hidden_units to 1 output neuron\n",
    "                            # no final tranfer function, i.e. \"linear output\"\n",
    "                            )\n",
    "        loss_fn = torch.nn.MSELoss() # notice how this is now a mean-squared-error loss\n",
    "        \n",
    "        # Extract training and test set for current CV fold, convert to tensors\n",
    "        X_train_i = torch.Tensor(X_train[train_index,:])\n",
    "        y_train_i = torch.Tensor(y_train[train_index])\n",
    "        X_test_i = torch.Tensor(X_train[test_index,:])\n",
    "        y_test_i = torch.Tensor(y_train[test_index])\n",
    "        \n",
    "        # Train the net on training data\n",
    "        net, final_loss, learning_curve = train_neural_net(model,\n",
    "                                                           loss_fn,\n",
    "                                                           X=X_train_i,\n",
    "                                                           y=y_train_i,\n",
    "                                                           n_replicates=n_replicates,\n",
    "                                                           max_iter=max_iter)\n",
    "        \n",
    "        print('\\n\\tBest loss: {}\\n'.format(final_loss))\n",
    "        \n",
    "        # Determine estimated class labels for test set\n",
    "        y_test_est = net(X_test_i)\n",
    "        \n",
    "        errors[k]=final_loss # store error rate for current CV fold \n",
    "    \n",
    "    minerror[k_o-1]=min(errors)\n",
    "    min_ind[k_o-1] = np.argmin(errors)\n",
    "    k_o += 1\n",
    "\n",
    "\n",
    "print(' ')\n",
    "print('------------------------------------')\n",
    "print('ANN model:')\n",
    "print(' ')    \n",
    "for ii in range(10):\n",
    "    print('For outer fold {} best generalization error is {} for h = {} '.format(ii+1,minerror[ii],min_ind[ii]+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\Jakob Højgaard\\\\OneDrive - Danmarks Tekniske Universitet\\\\DTU\\\\8. Semester\\\\02450 Introduction to Machine Learning and Data Mining\\\\project_1\\\\02450intro_machine_learning\\\\Project 2\\\\Scripts', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\python310.zip', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\DLLs', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\lib', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10', '', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\lib\\\\site-packages', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\lib\\\\site-packages\\\\win32', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\lib\\\\site-packages\\\\Pythonwin', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Jakob Højgaard\\\\.ipython', '../Tools']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Correlated t-test for linear regression and baseline\n",
      "p-value 0.012789825807308003 and confidence interval (-1.1167083413544194, -0.1739114710822698)\n",
      " \n",
      "Correlated t-test for ANN and baseline\n",
      "p-value 2.165393111676037e-05 and confidence interval (0.29755289921442724, 0.5312778098179999)\n",
      " \n",
      "Correlated t-test for linear regression and ANN\n",
      "p-value 0.0002782461866609748 and confidence interval (0.6424045789944546, 1.4770459424746618)\n"
     ]
    }
   ],
   "source": [
    "#%% Statistical test for method setup II\n",
    "\n",
    "# MSE are saved in variable:\n",
    "    # errors_lin_reg for linear regression\n",
    "    # error_o for baseline\n",
    "    # minerror for ANN\n",
    "\n",
    "# Generalization error differences\n",
    "r_lin_base = errors_lin_reg - error_o\n",
    "r_lin_ANN = minerror - errors_lin_reg\n",
    "r_ANN_base = minerror - error_o\n",
    "\n",
    "# significance level\n",
    "alpha = 0.05\n",
    "\n",
    "rho = 1/K\n",
    "\n",
    "print(' ')\n",
    "# Test lin reg vs baseline\n",
    "print('Correlated t-test for linear regression and baseline')\n",
    "p_val, conf_int = correlated_ttest(r_lin_base, rho, alpha)\n",
    "print('p-value {} and confidence interval {}'.format(p_val, conf_int))\n",
    "print(' ')\n",
    "\n",
    "# Test ANN vs baseline\n",
    "print('Correlated t-test for ANN and baseline')\n",
    "p_val, conf_int = correlated_ttest(r_ANN_base, rho, alpha)\n",
    "print('p-value {} and confidence interval {}'.format(p_val, conf_int))\n",
    "print(' ')\n",
    "\n",
    "# Test lin reg vs ANN\n",
    "print('Correlated t-test for linear regression and ANN')\n",
    "p_val, conf_int = correlated_ttest(r_lin_ANN, rho, alpha)\n",
    "print('p-value {} and confidence interval {}'.format(p_val, conf_int))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
