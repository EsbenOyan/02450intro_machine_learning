{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages and functions\n",
    "from matplotlib.pylab import figure, semilogx, loglog, xlabel, ylabel, legend, title, subplot, show, grid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn import model_selection\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Add the path to the toolbox_02450 module\n",
    "sys.path.append('../Tools')\n",
    "from toolbox_02450 import rlr_validate, correlated_ttest, train_neural_net, draw_neural_net\n",
    "\n",
    "# Import custom functions from data_preprocessing module\n",
    "from data_preprocessing import *\n",
    "dfjoint, dfRec, dfClas = dataPreprocess()\n",
    "colNamesMeans, colNamesStd, colNamesExt, colNamesOther = getSpecificColNames()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.7574074074074073\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_left = 33 + 28 + 30 + 29\n",
    "n_right = 4 + 2 + 3 + 5 + 1\n",
    "n_total = n_right + n_left\n",
    "\n",
    "p1 = (33 + 28 + 30 + 29)/n_total\n",
    "p2 = (4 + 2 + 3 + 5)/n_total\n",
    "p3 = 0\n",
    "p4 = 0.075\n",
    "\n",
    "lp = 1 - max(p1,p2,p3,p4)\n",
    "ll = 1 - 1/120\n",
    "lr = 1 - (n_left + n_right - 1) / 120\n",
    "\n",
    "print(lp - ((n_left / n_total) * ll) - ((n_right / n_total) * lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "mean = dfjoint.loc[:, colNamesMeans]\n",
    "sde = dfjoint.loc[:, colNamesStd]\n",
    "worst = dfjoint.loc[:, colNamesExt]\n",
    "colNames = colNamesMeans + colNamesStd + colNamesExt\n",
    "\n",
    "# Discretize time variable\n",
    "time_discretized = []\n",
    "for i in range(len(dfRec.iloc[:,0])):\n",
    "    if dfRec['time'].iloc[i] <= 12:\n",
    "        time_discretized.append('<1 years')\n",
    "    elif dfRec['time'].iloc[i] <= 36:\n",
    "        time_discretized.append('1-3 years')\n",
    "    elif dfRec['time'].iloc[i] <= 72:\n",
    "        time_discretized.append('>3-6 years')\n",
    "    else:\n",
    "        time_discretized.append('6+ years')\n",
    "\n",
    "# Add discretized time variable to dataframe\n",
    "dfRec['time_discretized'] = time_discretized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare data for classification\n",
    "attributeNames = [u'Offset'] + colNames\n",
    "X = dfRec.loc[:, colNames]\n",
    "classLabels = dfRec.loc[:,\"time_discretized\"]\n",
    "classNames = sorted(set(dfRec.loc[:,\"time_discretized\"]))\n",
    "\n",
    "# Create a dictionary mapping class names to integer labels\n",
    "classDict = dict(zip(classNames, range(4)))\n",
    "y = np.asarray([classDict[value] for value in classLabels])\n",
    "\n",
    "# Define data dimensions\n",
    "N = X.shape[0]\n",
    "M = len(attributeNames)\n",
    "# M = M + 1\n",
    "C = len(classNames)\n",
    "\n",
    "# Add bias to input data\n",
    "X = np.concatenate((np.ones((X.shape[0],1)), X), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "------------------------------------\n",
      "Linear regression model:\n",
      " \n",
      "Outer fold 1\n",
      "has optimal lambda: 1000.0\n",
      "and optimal generalization error: 1.2699102901348287\n",
      "Outer fold 2\n",
      "has optimal lambda: 100000000.0\n",
      "and optimal generalization error: 1.3625279831668704\n",
      "Outer fold 3\n",
      "has optimal lambda: 100000000.0\n",
      "and optimal generalization error: 1.3172608806155341\n",
      "Outer fold 4\n",
      "has optimal lambda: 100000000.0\n",
      "and optimal generalization error: 1.3124708660504683\n",
      "Outer fold 5\n",
      "has optimal lambda: 100000000.0\n",
      "and optimal generalization error: 1.341719254142665\n",
      "Outer fold 6\n",
      "has optimal lambda: 100000000.0\n",
      "and optimal generalization error: 1.309148923006171\n",
      "Outer fold 7\n",
      "has optimal lambda: 100000000.0\n",
      "and optimal generalization error: 1.3703819241338469\n",
      "Outer fold 8\n",
      "has optimal lambda: 100000000.0\n",
      "and optimal generalization error: 1.3569186376537685\n",
      "Outer fold 9\n",
      "has optimal lambda: 100000000.0\n",
      "and optimal generalization error: 1.2754145285900074\n",
      "Outer fold 10\n",
      "has optimal lambda: 100000000.0\n",
      "and optimal generalization error: 1.392459786684369\n"
     ]
    }
   ],
   "source": [
    "# 2 level cross validation, linear regression model\n",
    "\n",
    "# Define number of folds for cross-validation\n",
    "K = 10\n",
    "\n",
    "# Create cross-validation partition for evaluation\n",
    "CV = model_selection.KFold(K, shuffle=True)\n",
    "#CV = model_selection.KFold(K, shuffle=False)\n",
    "\n",
    "# Define lambda values to be tested\n",
    "lambdas = np.power(10.,range(-5,9))\n",
    "\n",
    "# Initialize variables to store errors and weights\n",
    "Error_train = np.empty((K,1))\n",
    "Error_test = np.empty((K,1))\n",
    "Error_train_rlr = np.empty((K,1))\n",
    "Error_test_rlr = np.empty((K,1))\n",
    "Error_train_nofeatures = np.empty((K,1))\n",
    "Error_test_nofeatures = np.empty((K,1))\n",
    "w_rlr = np.empty((M,K))\n",
    "mu = np.empty((K, M-1))\n",
    "sigma = np.empty((K, M-1))\n",
    "w_noreg = np.empty((M,K))\n",
    "errors_lin_reg = np.empty((K,1))\n",
    "\n",
    "# Loop over each fold in the cross-validation\n",
    "k = 0\n",
    "print(' ')\n",
    "print('------------------------------------')\n",
    "print('Linear regression model:')\n",
    "print(' ')\n",
    "for train_index, test_index in CV.split(X, y):\n",
    "\n",
    "    # Extract training and test set for current CV fold\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "    # Define the number of internal folds for cross-validation\n",
    "    internal_cross_validation = 10\n",
    "\n",
    "    # Validate the performance of the linear regression model using ridge regression\n",
    "    opt_val_err, opt_lambda, mean_w_vs_lambda, train_err_vs_lambda, test_err_vs_lambda = rlr_validate(X_train, y_train, lambdas, internal_cross_validation)\n",
    "\n",
    "    # Standardize the data based on the training set and save the mean and standard deviation\n",
    "    # since they're part of the model\n",
    "    mu[k, :] = np.mean(X_train[:, 1:], 0)\n",
    "    sigma[k, :] = np.std(X_train[:, 1:], 0)\n",
    "\n",
    "    X_train[:, 1:] = (X_train[:, 1:] - mu[k, :]) / sigma[k, :]\n",
    "    X_test[:, 1:] = (X_test[:, 1:] - mu[k, :]) / sigma[k, :]\n",
    "\n",
    "    # Save the generalization error for later statistics\n",
    "    errors_lin_reg[k] = opt_val_err\n",
    "\n",
    "    k += 1\n",
    "\n",
    "    # Print the results of the current fold\n",
    "    print('Outer fold {0}'.format(k))\n",
    "    print('has optimal lambda: {0}'.format(opt_lambda))\n",
    "    print('and optimal generalization error: {0}'.format(opt_val_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "------------------------------------\n",
      "Baseline model:\n",
      " \n",
      "Outer fold 1 generalization error: [0.91666667]\n",
      "Outer fold 2 generalization error: [0.90385455]\n",
      "Outer fold 3 generalization error: [1.02119091]\n",
      "Outer fold 4 generalization error: [0.83755455]\n",
      "Outer fold 5 generalization error: [1.03083333]\n",
      "Outer fold 6 generalization error: [0.93568367]\n",
      "Outer fold 7 generalization error: [0.74893333]\n",
      "Outer fold 8 generalization error: [0.75371399]\n",
      "Outer fold 9 generalization error: [0.97616989]\n",
      "Outer fold 10 generalization error: [0.7768668]\n"
     ]
    }
   ],
   "source": [
    "# 2 level Cross validation, baseline\n",
    "\n",
    "# Define number of outer and inner folds for cross-validation\n",
    "K = 10\n",
    "K_i = 10\n",
    "\n",
    "# Create cross-validation partition for evaluation\n",
    "CV = model_selection.KFold(K, shuffle=True)\n",
    "#CV = model_selection.KFold(K, shuffle=False)\n",
    "\n",
    "# Initialize variables to store results\n",
    "mu = np.empty((K, M-1))\n",
    "sigma = np.empty((K, M-1))\n",
    "error_i = np.empty((K_i, 1))\n",
    "error_o = np.empty((K, 1))\n",
    "\n",
    "\n",
    "print(' ')\n",
    "print('------------------------------------')\n",
    "print('Baseline model:')\n",
    "print(' ')\n",
    "# Loop over outer cross-validation folds\n",
    "k=0\n",
    "for train_index, test_index in CV.split(X,y):\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    \n",
    "    CV_i = model_selection.KFold(K_i, shuffle=True)\n",
    "    \n",
    "    k_i = 0\n",
    "    for train_index, test_index in CV_i.split(X_train,y_train):\n",
    "        \n",
    "        # extract training and test set for current inner CV fold\n",
    "        X_train_i = X_train[train_index]\n",
    "        y_train_i = y_train[train_index]\n",
    "        X_test_i = X_train[test_index]\n",
    "        y_test_i = y_train[test_index]\n",
    "        \n",
    "        mean_i = np.mean(y_train_i)\n",
    "        error_i[k_i]=(1/len(y_test_i))*np.sum((y_test_i-mean_i)**2)\n",
    "        #rykke error her op og find 10 inder errors\n",
    "        #Error_i[k_i] = (1/len(y_test_i)*\n",
    "        \n",
    "        k_i+=1\n",
    "    \n",
    "    error_o[k] = min(error_i)\n",
    "        \n",
    "    # Standardize outer fold based on training set, and save the mean and standard\n",
    "    # deviations since they're part of the model (they would be needed for\n",
    "    # making new predictions) - for brevity we won't always store these in the scripts\n",
    "    mu[k, :] = np.mean(X_train[:, 1:], 0)\n",
    "    sigma[k, :] = np.std(X_train[:, 1:], 0)\n",
    "    \n",
    "    X_train[:, 1:] = (X_train[:, 1:] - mu[k, :] ) / sigma[k, :] \n",
    "    X_test[:, 1:] = (X_test[:, 1:] - mu[k, :] ) / sigma[k, :] \n",
    "    \n",
    "    print('Outer fold {} generalization error: {}'.format(k+1,error_o[k]))\n",
    "\n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\py10\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([99])) that is different to the input size (torch.Size([99, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t1.6127735\t0.0006505521\n",
      "\t\t2000\t1.3156444\t1.1597824e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2335\t1.3135431\t9.98293e-07\n",
      "\n",
      "\tBest loss: 1.3135430812835693\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\py10\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t1.913898\t0.0010769494\n",
      "\t\t2000\t1.358823\t1.6142027e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2366\t1.3557918\t9.671845e-07\n",
      "\n",
      "\tBest loss: 1.3557918071746826\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t617\t1.3317014\t9.846809e-07\n",
      "\n",
      "\tBest loss: 1.331701397895813\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t559\t1.3275521\t9.877585e-07\n",
      "\n",
      "\tBest loss: 1.327552080154419\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t661\t1.3484602\t8.840393e-07\n",
      "\n",
      "\tBest loss: 1.3484601974487305\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t90\t1.417433\t1.6820445e-07\n",
      "\n",
      "\tBest loss: 1.4174330234527588\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t98\t1.3419073\t8.8835634e-07\n",
      "\n",
      "\tBest loss: 1.341907262802124\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t367\t1.2969183\t9.1917263e-07\n",
      "\n",
      "\tBest loss: 1.296918272972107\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t192\t1.3080162\t9.113739e-07\n",
      "\n",
      "\tBest loss: 1.3080161809921265\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t355\t1.3104218\t9.097008e-07\n",
      "\n",
      "\tBest loss: 1.3104218244552612\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.4140499\t0.00031721755\n",
      "\t\tFinal loss:\n",
      "\t\t1954\t1.3196244\t9.936925e-07\n",
      "\n",
      "\tBest loss: 1.319624423980713\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.279022\t1.2861914e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1212\t1.2777004\t9.3299786e-07\n",
      "\n",
      "\tBest loss: 1.277700424194336\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3083445\t8.382473e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1155\t1.3075967\t9.116663e-07\n",
      "\n",
      "\tBest loss: 1.3075966835021973\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t810\t1.2611638\t9.452315e-07\n",
      "\n",
      "\tBest loss: 1.2611638307571411\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3410593\t9.689122e-06\n",
      "\t\t2000\t1.3257762\t1.2138594e-05\n",
      "\t\t3000\t1.3109257\t9.639038e-06\n",
      "\t\t4000\t1.3016279\t4.5792176e-06\n",
      "\t\tFinal loss:\n",
      "\t\t4924\t1.2984865\t9.1806254e-07\n",
      "\n",
      "\tBest loss: 1.2984864711761475\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t484\t1.3600401\t9.641634e-07\n",
      "\n",
      "\tBest loss: 1.3600400686264038\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3731438\t0.00020839904\n",
      "\t\tFinal loss:\n",
      "\t\t1474\t1.3445964\t9.752375e-07\n",
      "\n",
      "\tBest loss: 1.3445963859558105\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t443\t1.2939299\t8.29166e-07\n",
      "\n",
      "\tBest loss: 1.293929934501648\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.4142296\t0.0005888593\n",
      "\t\tFinal loss:\n",
      "\t\t1594\t1.3212173\t9.924945e-07\n",
      "\n",
      "\tBest loss: 1.3212172985076904\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.300398\t2.8418049e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1642\t1.2988074\t9.1783573e-07\n",
      "\n",
      "\tBest loss: 1.2988073825836182\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.6383094\t0.0007717895\n",
      "\t\t2000\t1.2532951\t2.1115462e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2471\t1.2490281\t9.544154e-07\n",
      "\n",
      "\tBest loss: 1.2490280866622925\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.2464305\t0.00013091476\n",
      "\t\tFinal loss:\n",
      "\t\t1551\t1.2252182\t9.729628e-07\n",
      "\n",
      "\tBest loss: 1.2252181768417358\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t791\t1.327578\t8.9794486e-07\n",
      "\n",
      "\tBest loss: 1.3275779485702515\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t710\t1.2896531\t9.243508e-07\n",
      "\n",
      "\tBest loss: 1.2896530628204346\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.4895757\t0.00094225333\n",
      "\t\tFinal loss:\n",
      "\t\t1854\t1.2701219\t9.385648e-07\n",
      "\n",
      "\tBest loss: 1.2701219320297241\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3082309\t2.3144583e-05\n",
      "\t\t2000\t1.2730846\t2.958878e-05\n",
      "\t\t3000\t1.2381791\t2.416517e-05\n",
      "\t\t4000\t1.2161268\t1.1468645e-05\n",
      "\t\t5000\t1.2085625\t2.4659253e-06\n",
      "\t\tFinal loss:\n",
      "\t\t5283\t1.2079724\t9.868535e-07\n",
      "\n",
      "\tBest loss: 1.2079724073410034\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t177\t1.3075106\t8.2055374e-07\n",
      "\n",
      "\tBest loss: 1.3075106143951416\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t426\t1.2908224\t9.235134e-07\n",
      "\n",
      "\tBest loss: 1.2908223867416382\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.2517517\t1.5237413e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1254\t1.2513242\t8.573979e-07\n",
      "\n",
      "\tBest loss: 1.25132417678833\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3367026\t1.3109525e-05\n",
      "\t\t2000\t1.320428\t1.0021067e-05\n",
      "\t\t3000\t1.3116986\t3.6352517e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3690\t1.309705\t9.101987e-07\n",
      "\n",
      "\tBest loss: 1.3097050189971924\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t695\t1.2415756\t9.601443e-07\n",
      "\n",
      "\tBest loss: 1.2415755987167358\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.303182\t0.00014597367\n",
      "\t\tFinal loss:\n",
      "\t\t1578\t1.277237\t9.3333637e-07\n",
      "\n",
      "\tBest loss: 1.277237057685852\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t2.8811886\t0.0018218106\n",
      "\t\t2000\t1.2578673\t9.476198e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2659\t1.2377857\t9.630841e-07\n",
      "\n",
      "\tBest loss: 1.2377856969833374\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t17\t1.2075009\t1.9744799e-07\n",
      "\n",
      "\tBest loss: 1.20750093460083\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.2592324\t3.0293738e-06\n",
      "\t\t2000\t1.2541414\t4.7526028e-06\n",
      "\t\t3000\t1.2473322\t5.925392e-06\n",
      "\t\t4000\t1.24034\t5.1899224e-06\n",
      "\t\t5000\t1.2352054\t3.0883004e-06\n",
      "\t\tFinal loss:\n",
      "\t\t5825\t1.2331319\t9.667187e-07\n",
      "\n",
      "\tBest loss: 1.2331318855285645\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.2648243\t1.8472598e-05\n",
      "\t\t2000\t1.2500906\t5.1494417e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2609\t1.2479132\t9.552681e-07\n",
      "\n",
      "\tBest loss: 1.2479132413864136\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t149\t1.2259053\t5.834507e-07\n",
      "\n",
      "\tBest loss: 1.2259052991867065\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t196\t1.2571133\t8.534495e-07\n",
      "\n",
      "\tBest loss: 1.2571133375167847\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.2520671\t2.380193e-05\n",
      "\t\t2000\t1.2417183\t3.5521166e-06\n",
      "\t\t3000\t1.2375097\t3.08255e-06\n",
      "\t\t4000\t1.2345359\t1.8346751e-06\n",
      "\t\tFinal loss:\n",
      "\t\t4321\t1.233973\t9.660598e-07\n",
      "\n",
      "\tBest loss: 1.2339730262756348\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t203\t1.2156175\t9.80647e-07\n",
      "\n",
      "\tBest loss: 1.2156175374984741\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t2.0703816\t0.0009002909\n",
      "\t\t2000\t1.406846\t5.8463826e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2728\t1.390207\t9.4324145e-07\n",
      "\n",
      "\tBest loss: 1.390207052230835\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.6636015\t0.0007440376\n",
      "\t\t2000\t1.3849201\t3.3569781e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2129\t1.384567\t9.470838e-07\n",
      "\n",
      "\tBest loss: 1.3845670223236084\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\py10\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([101])) that is different to the input size (torch.Size([101, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t1000\t1.3814336\t3.9695037e-06\n",
      "\t\t2000\t1.3746121\t6.0705124e-06\n",
      "\t\t3000\t1.3658043\t6.8079016e-06\n",
      "\t\t4000\t1.3572656\t5.1819716e-06\n",
      "\t\t5000\t1.3515283\t2.8224983e-06\n",
      "\t\tFinal loss:\n",
      "\t\t5661\t1.3497292\t9.71529e-07\n",
      "\n",
      "\tBest loss: 1.3497291803359985\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t289\t1.3595004\t9.645462e-07\n",
      "\n",
      "\tBest loss: 1.3595004081726074\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t929\t1.4126822\t9.2823495e-07\n",
      "\n",
      "\tBest loss: 1.4126821756362915\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.4262264\t1.2788144e-05\n",
      "\t\t2000\t1.4052812\t1.5438729e-05\n",
      "\t\t3000\t1.3851348\t1.19626575e-05\n",
      "\t\t4000\t1.3729529\t5.643719e-06\n",
      "\t\tFinal loss:\n",
      "\t\t4931\t1.3691095\t9.577765e-07\n",
      "\n",
      "\tBest loss: 1.3691095113754272\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t247\t1.3896384\t8.578432e-07\n",
      "\n",
      "\tBest loss: 1.3896384239196777\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t525\t1.3546152\t9.680247e-07\n",
      "\n",
      "\tBest loss: 1.3546152114868164\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t326\t1.3245989\t8.999644e-07\n",
      "\n",
      "\tBest loss: 1.3245989084243774\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t116\t1.3486941\t8.83886e-07\n",
      "\n",
      "\tBest loss: 1.348694086074829\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t2.3122559\t0.0010604002\n",
      "\t\t2000\t1.3344598\t0.00013130005\n",
      "\t\tFinal loss:\n",
      "\t\t2962\t1.2926104\t9.22236e-07\n",
      "\n",
      "\tBest loss: 1.2926104068756104\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t726\t1.3244766\t9.900522e-07\n",
      "\n",
      "\tBest loss: 1.3244765996932983\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t777\t1.2817043\t9.3008333e-07\n",
      "\n",
      "\tBest loss: 1.281704306602478\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t597\t1.2728606\t9.365454e-07\n",
      "\n",
      "\tBest loss: 1.2728606462478638\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t570\t1.3246212\t8.999492e-07\n",
      "\n",
      "\tBest loss: 1.3246212005615234\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t629\t1.3334495\t9.8339e-07\n",
      "\n",
      "\tBest loss: 1.3334494829177856\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.2886788\t8.325385e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1526\t1.2860329\t9.269528e-07\n",
      "\n",
      "\tBest loss: 1.2860329151153564\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.326115\t4.3148757e-06\n",
      "\t\t2000\t1.3197877\t4.7871804e-06\n",
      "\t\t3000\t1.3148797\t2.6291839e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3630\t1.3132821\t9.984914e-07\n",
      "\n",
      "\tBest loss: 1.3132821321487427\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3274331\t4.669805e-06\n",
      "\t\t2000\t1.3210229\t4.4217472e-06\n",
      "\t\t3000\t1.3166549\t2.1729436e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3432\t1.3157299\t9.0603083e-07\n",
      "\n",
      "\tBest loss: 1.3157298564910889\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t61\t1.2498778\t5.722602e-07\n",
      "\n",
      "\tBest loss: 1.2498778104782104\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t588\t1.2875597\t8.332683e-07\n",
      "\n",
      "\tBest loss: 1.2875597476959229\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.4923868\t0.0006540945\n",
      "\t\t2000\t1.2882861\t1.9431943e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2080\t1.288139\t9.2543723e-07\n",
      "\n",
      "\tBest loss: 1.28813898563385\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.5253792\t0.00077949115\n",
      "\t\tFinal loss:\n",
      "\t\t1970\t1.3041253\t9.14093e-07\n",
      "\n",
      "\tBest loss: 1.3041253089904785\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.4347847\t0.0004886341\n",
      "\t\tFinal loss:\n",
      "\t\t1728\t1.3337284\t9.831843e-07\n",
      "\n",
      "\tBest loss: 1.3337284326553345\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3137008\t8.529779e-06\n",
      "\t\t2000\t1.3024759\t7.5965286e-06\n",
      "\t\t3000\t1.2951447\t3.4055877e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3735\t1.2930374\t8.297383e-07\n",
      "\n",
      "\tBest loss: 1.2930374145507812\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t317\t1.3063651\t8.2127326e-07\n",
      "\n",
      "\tBest loss: 1.3063651323318481\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t708\t1.3436487\t9.759254e-07\n",
      "\n",
      "\tBest loss: 1.3436486721038818\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3511834\t1.129278e-05\n",
      "\t\t2000\t1.3337532\t1.3853525e-05\n",
      "\t\t3000\t1.3168677\t1.0862867e-05\n",
      "\t\t4000\t1.3059428\t5.7507436e-06\n",
      "\t\t5000\t1.3009624\t2.4740475e-06\n",
      "\t\tFinal loss:\n",
      "\t\t5656\t1.2995497\t9.173114e-07\n",
      "\n",
      "\tBest loss: 1.2995496988296509\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t183\t1.3286664\t9.869301e-07\n",
      "\n",
      "\tBest loss: 1.3286664485931396\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3448064\t4.6981204e-06\n",
      "\t\t2000\t1.3371992\t6.5077975e-06\n",
      "\t\t3000\t1.3282077\t6.4621017e-06\n",
      "\t\t4000\t1.320244\t4.966111e-06\n",
      "\t\t5000\t1.3150843\t2.900716e-06\n",
      "\t\tFinal loss:\n",
      "\t\t5670\t1.3133551\t9.984359e-07\n",
      "\n",
      "\tBest loss: 1.3133550882339478\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t3.640789\t0.0011655246\n",
      "\t\t2000\t1.5187161\t0.00045944654\n",
      "\t\t3000\t1.2825041\t2.0727513e-05\n",
      "\t\tFinal loss:\n",
      "\t\t3569\t1.2773074\t9.33285e-07\n",
      "\n",
      "\tBest loss: 1.277307391166687\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t2.3322961\t0.001385881\n",
      "\t\t2000\t1.2966756\t6.747541e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2664\t1.2812686\t9.303996e-07\n",
      "\n",
      "\tBest loss: 1.28126859664917\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.5250207\t0.00065314386\n",
      "\t\tFinal loss:\n",
      "\t\t1910\t1.3488277\t9.721782e-07\n",
      "\n",
      "\tBest loss: 1.3488277196884155\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.9545165\t0.0015386452\n",
      "\t\t2000\t1.3056682\t5.9345534e-06\n",
      "\t\tFinal loss:\n",
      "\t\t2196\t1.3049147\t7.3083214e-07\n",
      "\n",
      "\tBest loss: 1.3049147129058838\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.4088703\t0.00010034141\n",
      "\t\t2000\t1.3845851\t1.0245495e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2159\t1.3220915\t9.918383e-07\n",
      "\n",
      "\tBest loss: 1.3220914602279663\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3219184\t2.5250058e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1162\t1.3215388\t9.92253e-07\n",
      "\n",
      "\tBest loss: 1.3215388059616089\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t561\t1.2542351\t7.603627e-07\n",
      "\n",
      "\tBest loss: 1.2542351484298706\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3217283\t6.8545432e-06\n",
      "\t\t2000\t1.3110864\t8.819556e-06\n",
      "\t\t3000\t1.3000357\t7.519092e-06\n",
      "\t\t4000\t1.2926271\t3.5044422e-06\n",
      "\t\tFinal loss:\n",
      "\t\t4862\t1.2900722\t9.240504e-07\n",
      "\n",
      "\tBest loss: 1.2900722026824951\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3308657\t1.5406275e-05\n",
      "\t\t2000\t1.3124769\t1.0990034e-05\n",
      "\t\t3000\t1.3034229\t3.4754162e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3567\t1.3019047\t9.1565215e-07\n",
      "\n",
      "\tBest loss: 1.3019046783447266\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t368\t1.3315238\t7.1622725e-07\n",
      "\n",
      "\tBest loss: 1.3315237760543823\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t101\t1.3475106\t9.731285e-07\n",
      "\n",
      "\tBest loss: 1.347510576248169\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t77\t1.2979257\t9.184592e-07\n",
      "\n",
      "\tBest loss: 1.2979257106781006\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.327593\t1.3199478e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1199\t1.3262461\t9.887311e-07\n",
      "\n",
      "\tBest loss: 1.3262461423873901\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t899\t1.2954427\t8.2819776e-07\n",
      "\n",
      "\tBest loss: 1.2954427003860474\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3470664\t3.8052913e-06\n",
      "\t\t2000\t1.3418229\t4.708567e-06\n",
      "\t\t3000\t1.3342403\t6.5222293e-06\n",
      "\t\t4000\t1.32517\t6.926697e-06\n",
      "\t\t5000\t1.3166578\t5.613406e-06\n",
      "\t\t6000\t1.3110435\t2.9096564e-06\n",
      "\t\tFinal loss:\n",
      "\t\t6694\t1.3092515\t8.194626e-07\n",
      "\n",
      "\tBest loss: 1.3092515468597412\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t107\t1.3855532\t8.603725e-07\n",
      "\n",
      "\tBest loss: 1.385553240776062\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3186588\t1.5639287e-05\n",
      "\t\tFinal loss:\n",
      "\t\t1571\t1.3140508\t9.979073e-07\n",
      "\n",
      "\tBest loss: 1.3140507936477661\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3520799\t6.700673e-06\n",
      "\t\t2000\t1.3422091\t7.2828375e-06\n",
      "\t\t3000\t1.3343602\t4.466888e-06\n",
      "\t\t4000\t1.3309387\t1.1643804e-06\n",
      "\t\tFinal loss:\n",
      "\t\t4012\t1.3309203\t9.852588e-07\n",
      "\n",
      "\tBest loss: 1.3309203386306763\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t248\t1.3268855\t8.9841353e-07\n",
      "\n",
      "\tBest loss: 1.326885461807251\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t57\t1.3410463\t9.778212e-07\n",
      "\n",
      "\tBest loss: 1.3410463333129883\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t116\t1.3531116\t9.691004e-07\n",
      "\n",
      "\tBest loss: 1.3531116247177124\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.8629906\t0.0010651625\n",
      "\t\t2000\t1.3359183\t1.4991082e-05\n",
      "\t\tFinal loss:\n",
      "\t\t2358\t1.3332716\t9.835212e-07\n",
      "\n",
      "\tBest loss: 1.3332716226577759\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3240608\t3.1511483e-06\n",
      "\t\tFinal loss:\n",
      "\t\t1071\t1.3238826\t9.904963e-07\n",
      "\n",
      "\tBest loss: 1.3238825798034668\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3349769\t0.0001897199\n",
      "\t\tFinal loss:\n",
      "\t\t1533\t1.3056633\t9.130162e-07\n",
      "\n",
      "\tBest loss: 1.3056633472442627\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t886\t1.2926822\t9.2218477e-07\n",
      "\n",
      "\tBest loss: 1.29268217086792\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t769\t1.2454232\t7.657426e-07\n",
      "\n",
      "\tBest loss: 1.2454231977462769\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3620813\t1.0764838e-05\n",
      "\t\t2000\t1.349585\t7.0663764e-06\n",
      "\t\t3000\t1.3426589\t3.640212e-06\n",
      "\t\t4000\t1.3385284\t2.4936728e-06\n",
      "\t\tFinal loss:\n",
      "\t\t4898\t1.336347\t9.812578e-07\n",
      "\n",
      "\tBest loss: 1.336346983909607\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t140\t1.3237995\t9.905585e-07\n",
      "\n",
      "\tBest loss: 1.32379949092865\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\t1000\t1.3215338\t4.7808553e-06\n",
      "\t\t2000\t1.3152289\t4.4412263e-06\n",
      "\t\t3000\t1.310829\t2.2735423e-06\n",
      "\t\tFinal loss:\n",
      "\t\t3460\t1.3098353\t9.1010816e-07\n",
      "\n",
      "\tBest loss: 1.3098353147506714\n",
      "\n",
      "\n",
      "\tReplicate: 1/1\n",
      "\t\tIter\tLoss\t\t\tRel. loss\n",
      "\t\tFinal loss:\n",
      "\t\t94\t1.301841\t4.5784867e-07\n",
      "\n",
      "\tBest loss: 1.3018410205841064\n",
      "\n",
      " \n",
      "------------------------------------\n",
      "ANN model:\n",
      " \n",
      "For outer fold 1 best generalization error is [1.29691827] for h = [8.] \n",
      "For outer fold 2 best generalization error is [1.26116383] for h = [4.] \n",
      "For outer fold 3 best generalization error is [1.20797241] for h = [6.] \n",
      "For outer fold 4 best generalization error is [1.20750093] for h = [4.] \n",
      "For outer fold 5 best generalization error is [1.32459891] for h = [9.] \n",
      "For outer fold 6 best generalization error is [1.24987781] for h = [10.] \n",
      "For outer fold 7 best generalization error is [1.28755975] for h = [1.] \n",
      "For outer fold 8 best generalization error is [1.25423515] for h = [7.] \n",
      "For outer fold 9 best generalization error is [1.2954427] for h = [4.] \n",
      "For outer fold 10 best generalization error is [1.2454232] for h = [6.] \n"
     ]
    }
   ],
   "source": [
    "# ANN\n",
    "\n",
    "# Create crossvalidation partition for evaluation\n",
    "K = 10\n",
    "K_i = 10\n",
    "CV = model_selection.KFold(K, shuffle=True)\n",
    "#CV = model_selection.KFold(K, shuffle=False)\n",
    "\n",
    "# Initialize variables\n",
    "mu = np.empty((K, M-1))\n",
    "sigma = np.empty((K, M-1))\n",
    "error_i = np.empty((K_i,1))\n",
    "\n",
    "minerror=np.empty((K,1))\n",
    "min_ind=np.empty((K,1))\n",
    "\n",
    "\n",
    "k_o=1\n",
    "for train_index, test_index in CV.split(X,y):\n",
    "    \n",
    "    # extract training and test set for current CV fold\n",
    "    X_train = X[train_index]\n",
    "    y_train = y[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_test = y[test_index]\n",
    "\n",
    "\n",
    "    # Parameters for neural network classifier\n",
    "    # K-fold crossvalidation\n",
    "    CV = model_selection.KFold(K_i, shuffle=True)\n",
    "    \n",
    "    #print('Training model of type:\\n\\n{}\\n'.format(str(model())))\n",
    "    errors = np.empty((K,1)) # make a list for storing generalizaition error in each loop\n",
    "    for (k, (train_index, test_index)) in enumerate(CV.split(X_train,y_train)): \n",
    "        #print('\\nCrossvalidation fold: {0}/{1}'.format(k+1,K_i))  \n",
    "        n_hidden_units = k+1      # number of hidden units\n",
    "        n_replicates = 1        # number of networks trained in each k-fold\n",
    "        max_iter = 10000\n",
    "        \n",
    "        # Define the model\n",
    "        model = lambda: torch.nn.Sequential(\n",
    "                            torch.nn.Linear(M, n_hidden_units), #M features to n_hidden_units\n",
    "                            torch.nn.Tanh(),   # 1st transfer function,\n",
    "                            torch.nn.Linear(n_hidden_units, 1), # n_hidden_units to 1 output neuron\n",
    "                            # no final tranfer function, i.e. \"linear output\"\n",
    "                            )\n",
    "        loss_fn = torch.nn.MSELoss() # notice how this is now a mean-squared-error loss\n",
    "        \n",
    "        # Extract training and test set for current CV fold, convert to tensors\n",
    "        X_train_i = torch.Tensor(X_train[train_index,:])\n",
    "        y_train_i = torch.Tensor(y_train[train_index])\n",
    "        X_test_i = torch.Tensor(X_train[test_index,:])\n",
    "        y_test_i = torch.Tensor(y_train[test_index])\n",
    "        \n",
    "        # Train the net on training data\n",
    "        net, final_loss, learning_curve = train_neural_net(model,\n",
    "                                                           loss_fn,\n",
    "                                                           X=X_train_i,\n",
    "                                                           y=y_train_i,\n",
    "                                                           n_replicates=n_replicates,\n",
    "                                                           max_iter=max_iter)\n",
    "        \n",
    "        print('\\n\\tBest loss: {}\\n'.format(final_loss))\n",
    "        \n",
    "        # Determine estimated class labels for test set\n",
    "        y_test_est = net(X_test_i)\n",
    "        \n",
    "        errors[k]=final_loss # store error rate for current CV fold \n",
    "    \n",
    "    minerror[k_o-1]=min(errors)\n",
    "    min_ind[k_o-1] = np.argmin(errors)\n",
    "    k_o += 1\n",
    "\n",
    "\n",
    "print(' ')\n",
    "print('------------------------------------')\n",
    "print('ANN model:')\n",
    "print(' ')    \n",
    "for ii in range(10):\n",
    "    print('For outer fold {} best generalization error is {} for h = {} '.format(ii+1,minerror[ii],min_ind[ii]+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\Jakob Højgaard\\\\OneDrive - Danmarks Tekniske Universitet\\\\DTU\\\\8. Semester\\\\02450 Introduction to Machine Learning and Data Mining\\\\project_1\\\\02450intro_machine_learning\\\\Project 2\\\\Scripts', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\python310.zip', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\DLLs', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\lib', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10', '', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\lib\\\\site-packages', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\lib\\\\site-packages\\\\win32', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\lib\\\\site-packages\\\\Pythonwin', 'c:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\py10\\\\lib\\\\site-packages\\\\IPython\\\\extensions', 'C:\\\\Users\\\\Jakob Højgaard\\\\.ipython', '../Tools']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Correlated t-test for linear regression and baseline\n",
      "p-value 0.012789825807308003 and confidence interval (-1.1167083413544194, -0.1739114710822698)\n",
      " \n",
      "Correlated t-test for ANN and baseline\n",
      "p-value 2.165393111676037e-05 and confidence interval (0.29755289921442724, 0.5312778098179999)\n",
      " \n",
      "Correlated t-test for linear regression and ANN\n",
      "p-value 0.0002782461866609748 and confidence interval (0.6424045789944546, 1.4770459424746618)\n"
     ]
    }
   ],
   "source": [
    "#%% Statistical test for method setup II\n",
    "\n",
    "# MSE are saved in variable:\n",
    "    # errors_lin_reg for linear regression\n",
    "    # error_o for baseline\n",
    "    # minerror for ANN\n",
    "\n",
    "# Generalization error differences\n",
    "r_lin_base = errors_lin_reg - error_o\n",
    "r_lin_ANN = minerror - errors_lin_reg\n",
    "r_ANN_base = minerror - error_o\n",
    "\n",
    "# significance level\n",
    "alpha = 0.05\n",
    "\n",
    "rho = 1/K\n",
    "\n",
    "print(' ')\n",
    "# Test lin reg vs baseline\n",
    "print('Correlated t-test for linear regression and baseline')\n",
    "p_val, conf_int = correlated_ttest(r_lin_base, rho, alpha)\n",
    "print('p-value {} and confidence interval {}'.format(p_val, conf_int))\n",
    "print(' ')\n",
    "\n",
    "# Test ANN vs baseline\n",
    "print('Correlated t-test for ANN and baseline')\n",
    "p_val, conf_int = correlated_ttest(r_ANN_base, rho, alpha)\n",
    "print('p-value {} and confidence interval {}'.format(p_val, conf_int))\n",
    "print(' ')\n",
    "\n",
    "# Test lin reg vs ANN\n",
    "print('Correlated t-test for linear regression and ANN')\n",
    "p_val, conf_int = correlated_ttest(r_lin_ANN, rho, alpha)\n",
    "print('p-value {} and confidence interval {}'.format(p_val, conf_int))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dd5f8e12b52256db706a7757d4ff925217ebc162bb7ba53c9abf8d847d6b988"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
